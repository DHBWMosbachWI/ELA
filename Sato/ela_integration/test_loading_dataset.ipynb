{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enviroment set-up\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"D:\\\\20120321_DHBW_AZUREML\\\\sato\")\n",
    "sys.path.append(\"D:\\\\20120321_DHBW_AZUREML\\\\sato\\\\model\")\n",
    "from os.path import join\n",
    "\n",
    "# set env-var\n",
    "os.environ['BASEPATH'] = 'D:\\\\20120321_DHBW_AZUREML\\\\sato'\n",
    "os.environ['RAW_DIR'] = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/dhbwmlc-ds12-v2/code/Users/svenlangenecker/viznet-master/raw' # path to the raw data\n",
    "os.environ['SHERLOCKPATH'] = os.environ['BASEPATH']+'\\\\sherlock'\n",
    "os.environ['EXTRACTPATH'] = os.environ['BASEPATH']+'\\\\extract'\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['SHERLOCKPATH']\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['BASEPATH']\n",
    "os.environ['TYPENAME'] = 'type78'\n",
    "\n",
    "\n",
    "from model import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils import get_valid_types\n",
    "import copy\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(get_valid_types(os.environ[\"TYPENAME\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data in train_sherlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "column_level_split_file_path = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_1_absolute_20.0.json\"\n",
    "#embclus_gen_training_data_path = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\emb_clus\\\\knn_classifier\\\\out\\\\gen_training_data\\\\gen_training_data_1_1e-06_10_70_20.csv\"\n",
    "with open(column_level_split_file_path) as f:\n",
    "    labeled_unlabeled_test_split_file = json.load(f)\n",
    "\n",
    "#embclus_gen_training_data_path = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\emb_clus\\\\knn_classifier\\\\out\\\\gen_training_data\\\\gen_training_data_1_1e-06_10_70_20.csv\"\n",
    "#df_embclus_training_data = pd.read_csv(embclus_gen_training_data_path)\n",
    "#df_embclus_training_data[\"field_name\"] = label_enc.transform(df_embclus_training_data[\"predicted_semantic_type\"].tolist())\n",
    "#df_embclus_training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data in train_CRF_LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from model import models_sherlock\n",
    "from utils import name2dic\n",
    "\n",
    "topic = \"num-directstr_thr-0_tn-400\"\n",
    "\n",
    "if topic:\n",
    "    topic_dim = int(name2dic(topic)['tn'])\n",
    "else:\n",
    "    topic_dim = None\n",
    "\n",
    "# load single column model\n",
    "# pre_trained_sherlock_loc = join(\n",
    "#     os.environ['BASEPATH'], 'model', 'pre_trained_sherlock', os.environ[\"TYPENAME\"])\n",
    "# classifier = models_sherlock.build_sherlock(['char', 'rest', 'par', 'word'], num_classes=len(\n",
    "#     get_valid_types(os.environ[\"TYPENAME\"])), topic_dim=topic_dim, dropout_ratio=0.35).to(device)\n",
    "\n",
    "# classifier.load_state_dict(torch.load(\n",
    "#             join(pre_trained_sherlock_loc, \"all_None.pt\"), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type78\n",
      "D:\\20120321_DHBW_AZUREML\\sato\\tmp\\public_bi_benchmark_type78_header_valid.pkl pickle file found, loading...\n",
      "public_bi_benchmark_type78_header_valid Load complete. Time 0.01585865020751953\n",
      "Total data preparation time: 0.15108585357666016\n"
     ]
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(get_valid_types(os.environ[\"TYPENAME\"]))\n",
    "print(os.environ[\"TYPENAME\"])\n",
    "\n",
    "whole_corpus = datasets.TableFeatures(\"public_bi_benchmark\",['char', 'rest', 'par', 'word'], topic_feature=topic, label_enc=label_enc, id_filter=None, max_col_count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field_list       [1, 2, 10, 14, 20, 22, 23, 29, 30]\n",
       "field_names    [70, 12, 37, 43, 62, 22, 70, 46, 46]\n",
       "Name: CityMaxCapita+CityMaxCapita_1, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_corpus.df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for splitting on column-level: 0.003998994827270508\n"
     ]
    }
   ],
   "source": [
    "train_list = []\n",
    "#train_1 = \n",
    "#train = copy.copy(whole_corpus).set_filter_for_column_level_split(labeled_unlabeled_test_split_file[\"labeled1\"]+labeled_unlabeled_test_split_file[\"unlabeled\"])\n",
    "train = copy.copy(whole_corpus).set_filter_for_column_level_split(['CityMaxCapita_1+column_1', \"CityMaxCapita_1+column_2\"])\n",
    "\n",
    "train_list.append(train)\n",
    "\n",
    "training_data = ConcatDataset(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field_list       [1, 2]\n",
       "field_names    [70, 12]\n",
       "Name: CityMaxCapita+CityMaxCapita_1, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.overwrite_col_labels_from_embclus(pd.DataFrame({\"table\":[\"CityMaxCapita_1\",\"CityMaxCapita_1\"], \"column\":[\"column_1\",\"column_2\"], \"dataset_id\":[\"CityMaxCapita_1+column_1\",\"CityMaxCapita_1+column_2\"], \"predicted_semantic_type\":[\"country\", \"country\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field_list       [1, 2]\n",
       "field_names    [22, 22]\n",
       "Name: CityMaxCapita+CityMaxCapita_1, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to convert to SherlockDataset (column features) 0.0019998550415039062\n"
     ]
    }
   ],
   "source": [
    "sherlock_train = train.to_col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CityMaxCapita+CityMaxCapita_1': tensor([[ 7.5694e+00,  5.0400e-01,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  9.2640e+00,  3.7617e+00,  0.0000e+00,  0.0000e+00,\n",
       "           2.0780e+00,  6.8842e-01,  1.0000e+03,  1.0000e+00,  1.0000e+00,\n",
       "           1.1165e+01,  2.1946e+01,  3.0000e+00,  2.8000e+01,  1.2000e+01,\n",
       "           1.1165e+04, -2.5186e-01, -4.1184e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 7.7183e+00,  5.2000e-01,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  7.6910e+00,  3.1559e+00,  0.0000e+00,  0.0000e+00,\n",
       "           1.2660e+00,  4.8088e-01,  1.0000e+03,  1.0000e+00,  1.0000e+00,\n",
       "           7.9640e+00,  1.2093e+01,  3.0000e+00,  2.8000e+01,  8.0000e+00,\n",
       "           7.9640e+03,  1.3677e+00,  6.4430e-01,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00]])}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.data_dic[\"rest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.7183e+00, 5.2000e-01, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        7.6910e+00, 3.1559e+00, 0.0000e+00, 0.0000e+00, 1.2660e+00, 4.8088e-01,\n",
       "        1.0000e+03, 1.0000e+00, 1.0000e+00, 7.9640e+00, 1.2093e+01, 3.0000e+00,\n",
       "        2.8000e+01, 8.0000e+00, 7.9640e+03, 1.3677e+00, 6.4430e-01, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sherlock_train.__getitem__(1)[\"data\"][\"rest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]\n",
    "train.df_header\n",
    "df_header = copy.copy(train.df_header)\n",
    "#df_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_id</th>\n",
       "      <th>field_list</th>\n",
       "      <th>field_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_id</td>\n",
       "      <td>[0, 1]</td>\n",
       "      <td>[67, 77]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  table_id field_list field_names\n",
       "0  test_id     [0, 1]    [67, 77]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = []\n",
    "new_data = [\"test_id\", [0,1], [67,77]]\n",
    "test_list.append(new_data)\n",
    "pd.DataFrame(test_list, columns=[\"table_id\", \"field_list\", \"field_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_header = copy.copy(train.df_header).values.tolist()\n",
    "for i in range(1000):\n",
    "    df_header.append(df_header[0])\n",
    "df = pd.DataFrame(df_header, columns=[\"1\",\"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_header = copy.copy(train.df_header)\n",
    "for i in range(1000):\n",
    "    df_header = pd.concat([df_header, df_header.iloc[1]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_header = copy.copy(train.df_header)\n",
    "for i in range(1000):\n",
    "    df_header = df_header.append(df_header.iloc[1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_embclus_training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-10c187935449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverwrite_col_labels_from_embclus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_embclus_training_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_embclus_training_data' is not defined"
     ]
    }
   ],
   "source": [
    "train = train.overwrite_col_labels_from_embclus(df_embclus_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_header' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9a46b03a6fea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_header\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"CityMaxCapita+CityMaxCapita_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_header' is not defined"
     ]
    }
   ],
   "source": [
    "df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "field_list       [1, 2, 10, 14, 20, 22, 23, 30]\n",
       "field_names    [70, 12, 37, 43, 62, 22, 70, 46]\n",
       "Name: CityMaxCapita+CityMaxCapita_1, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_embclus_training_data.iterrows():\n",
    "    # if index > 2:\n",
    "    #     break\n",
    "    #print(index)\n",
    "    dataset_id = row[\"table\"]\n",
    "    locator = dataset_id.split(\"_\")[0]\n",
    "    table_id = locator+\"+\"+dataset_id\n",
    "    column_id = int(row[\"column\"].split(\"_\")[1])\n",
    "    \n",
    "    field_list = eval(df_header.loc[table_id][\"field_list\"])\n",
    "    field_names = eval(df_header.loc[table_id][\"field_names\"])\n",
    "\n",
    "    # selection in df_header\n",
    "    #print(df_header.loc[table_id])\n",
    "    #print(field_list.index(column_id))\n",
    "    searched_index = field_list.index(column_id)\n",
    "\n",
    "    # override field_names with the predicted semantic type\n",
    "    field_names[searched_index] = row[\"field_name\"]\n",
    "\n",
    "    #print(field_names)\n",
    "\n",
    "    df_header.loc[table_id][\"field_names\"] = str(field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_header.loc[\"CityMaxCapita+CityMaxCapita_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "train = copy.copy(whole_corpus).set_filter([\n",
    "    \"CommonGovernment+CommonGovernment_10\"\n",
    "])\n",
    "\n",
    "test = copy.copy(whole_corpus).set_filter([\n",
    "    #\"MLB+MLB_1\",\n",
    "    \"MLB+MLB_46\"\n",
    "])\n",
    "\n",
    "train_list.append(train)\n",
    "\n",
    "training_data = ConcatDataset(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_worker = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "training = datasets.generate_batches(training_data,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 shuffle=False,\n",
    "                                                 drop_last=True,\n",
    "                                                 device=device,\n",
    "                                                 n_workers=n_worker)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import classification_report\n",
    "from torchcrf import CRF\n",
    "y_pred, y_true = [], []\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "training_iter = tqdm(training, desc=\"Training\")\n",
    "for table_batch, label_batch, mask_batch in training_iter:\n",
    "    for f_g in table_batch:\n",
    "        table_batch[f_g] = table_batch[f_g].view(batch_size * 100, -1)\n",
    "\n",
    "    pred_scores = classifier.predict(table_batch)\n",
    "    #print(pred_scores[0])\n",
    "    pred = torch.argmax(pred_scores, dim=1).cpu().numpy()\n",
    "    #print(pred)\n",
    "\n",
    "    labels = label_batch.view(-1).cpu().numpy()\n",
    "    masks = mask_batch.view(-1).cpu().numpy()\n",
    "    invert_masks = np.invert(masks == 1)\n",
    "    #print(labels)\n",
    "\n",
    "    y_pred.extend(ma.array(pred, mask=invert_masks).compressed())\n",
    "    y_true.extend(ma.array(labels, mask=invert_masks).compressed())\n",
    "\n",
    "    print(\"Predictions: \",y_pred)\n",
    "    print(\"True types: \",y_true)\n",
    "    #print(classification_report(y_true, y_pred))\n",
    "\n",
    "    emissions = classifier(table_batch).view(\n",
    "                    batch_size, 100, -1).to(device)\n",
    "    #print(\"Emissions: \",emissions)\n",
    "\n",
    "    model = CRF(len(get_valid_types(os.environ[\"TYPENAME\"])), batch_first=True).to(device)\n",
    "    # load pretrained model\n",
    "    model_loc = join(os.environ['BASEPATH'], 'model',\n",
    "                        'pre_trained_CRF', os.environ[\"TYPENAME\"])\n",
    "    loaded_params = torch.load(\n",
    "        join(model_loc, \"CRF+LDA_pathL.pt\"), map_location=device)\n",
    "    model.load_state_dict(loaded_params['CRF_model'])\n",
    "    model.eval()\n",
    "\n",
    "    pred = model.decode(emissions, mask_batch)\n",
    "    print(\"SATO Prediction:\")\n",
    "    print(pred)\n",
    "\n",
    "    print(classification_report(y_true, pred[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "labels = label_batch.view(-1).cpu().numpy()\n",
    "masks = mask_batch.view(-1).cpu().numpy()\n",
    "invert_masks = np.invert(masks == 1)\n",
    "\n",
    "ma.array(labels, mask=invert_masks).compressed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_batch[0][0] = 1\n",
    "mask_batch[0][1] = 1\n",
    "mask_batch[0][2] = 0\n",
    "mask_batch[0][3] = 0\n",
    "mask_batch[0][4] = 0\n",
    "print(mask_batch)\n",
    "print(mask_batch[0].all())\n",
    "print(mask_batch[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_loading_dataset.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# look in table topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract.helpers.read_raw_data import get_filtered_dfs_by_corpus\n",
    "from extract.helpers.utils import valid_header_iter_gen\n",
    "\n",
    "\n",
    "corpus = \"public_bi_benchmark\"\n",
    "TYPENAME = \"type78\"\n",
    "header_name = \"{}_{}_header_valid.csv\".format(corpus, TYPENAME)\n",
    "header_iter = valid_header_iter_gen(header_name)\n",
    "\n",
    "print(header_iter)\n",
    "#raw_df_iter = get_filtered_dfs_by_corpus[\"publicbibenchmark\"](header_iter)\n",
    "\n",
    "for df_features in header_iter:\n",
    "    print(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "sherlock_feature_groups = ['char', 'word', 'par', 'rest']\n",
    "other_feature_groups = ['topic']\n",
    "column_level_id_filter = [\n",
    "    \"MLB_1+column_37\",\n",
    "    \"MLB_1+column_45\", \n",
    "    \"MLB_46+column_52\",\n",
    "    \"MLB_46+column_58\"]\n",
    "\n",
    "column_level_id_filter = labeled_unlabeled_test_split_file[\"test20.0\"]+labeled_unlabeled_test_split_file[\"unlabeled\"]\n",
    "\n",
    "df_dic = {}\n",
    "\n",
    "for training_set in column_level_id_filter:\n",
    "    dataset_id = training_set.split(\"+\")[0]\n",
    "    # this locator is only suitable for public_bi corpus\n",
    "    locator = dataset_id.split(\"_\")[0]\n",
    "    table_id = locator+\"+\"+dataset_id\n",
    "    column_id = int(training_set.split(\"+\")[1].split(\"_\")[1])\n",
    "    if table_id not in df_dic.keys():\n",
    "        df_dic[table_id] = [column_id]\n",
    "    elif table_id in df_dic.keys():\n",
    "        df_dic[table_id].append(column_id)\n",
    "\n",
    "#print(df_dic)\n",
    "\n",
    "# generate new df_header\n",
    "df_header_new = pd.DataFrame(\n",
    "    {\"table_id\": [], \"field_list\": [], \"field_names\": []})\n",
    "## manipulate data_dic\n",
    "old_data_dic = whole_corpus.data_dic\n",
    "new_data_dic = {}\n",
    "for f_g in old_data_dic.keys():\n",
    "    new_data_dic[f_g] = {}\n",
    "for f_g in sherlock_feature_groups:\n",
    "    new_data_dic[f_g] = {}\n",
    "for f_g in other_feature_groups:\n",
    "    new_data_dic[f_g] = {}\n",
    "old_data_dic = whole_corpus.data_dic\n",
    "\n",
    "for table_id in df_dic.keys():\n",
    "    #print(table_id)\n",
    "    # sort every field_list\n",
    "    df_dic[table_id] = sorted(df_dic[table_id])\n",
    "\n",
    "    old_field_list = np.array(eval(\n",
    "        whole_corpus.df_header.loc[table_id][\"field_list\"]))\n",
    "    #print(old_field_list)\n",
    "\n",
    "    old_field_names = np.array(eval(\n",
    "        whole_corpus.df_header.loc[table_id][\"field_names\"]))\n",
    "\n",
    "    new_field_list = df_dic[table_id]\n",
    "    mask_fields = np.array([x in new_field_list for x in old_field_list])\n",
    "    new_field_names = old_field_names[mask_fields].tolist()\n",
    "\n",
    "    df_header_new = df_header_new.append(pd.DataFrame({\"table_id\": [table_id], \"field_list\": [\n",
    "                                            str(new_field_list)], \"field_names\": [str(new_field_names)]}), ignore_index=True)\n",
    "\n",
    "    \n",
    "    for f_g in old_data_dic.keys():\n",
    "        new_data_dic[f_g][table_id] = old_data_dic[f_g][table_id][np.argwhere(mask_fields == True).flatten()]\n",
    "    for f_g in sherlock_feature_groups:\n",
    "        new_data_dic[f_g][table_id] = old_data_dic[f_g][table_id][np.argwhere(\n",
    "            mask_fields == True).flatten()]\n",
    "    for f_g in other_feature_groups:\n",
    "        new_data_dic[f_g][table_id] = old_data_dic[f_g][table_id]\n",
    "    \n",
    "    \n",
    "df_header_new = df_header_new.set_index(\"table_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dic[\"rest\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building NN with different number of types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SherlockClassifier(\n",
      "  (FeatureEncoder_char): FeatureEncoder(\n",
      "    (bn1): BatchNorm1d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear1): Linear(in_features=960, out_features=300, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (dp1): Dropout(p=0.35, inplace=False)\n",
      "    (linear2): Linear(in_features=300, out_features=300, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (FeatureEncoder_rest): FeatureEncoder(\n",
      "    (bn1): BatchNorm1d(27, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear1): Linear(in_features=27, out_features=27, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (dp1): Dropout(p=0.35, inplace=False)\n",
      "    (linear2): Linear(in_features=27, out_features=27, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (FeatureEncoder_par): FeatureEncoder(\n",
      "    (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear1): Linear(in_features=400, out_features=400, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (dp1): Dropout(p=0.35, inplace=False)\n",
      "    (linear2): Linear(in_features=400, out_features=400, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (FeatureEncoder_word): FeatureEncoder(\n",
      "    (bn1): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear1): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (dp1): Dropout(p=0.35, inplace=False)\n",
      "    (linear2): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (FeatureEncoder_topic): FeatureEncoder(\n",
      "    (bn1): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (linear1): Linear(in_features=400, out_features=400, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (dp1): Dropout(p=0.35, inplace=False)\n",
      "    (linear2): Linear(in_features=400, out_features=400, bias=True)\n",
      "    (relu2): ReLU()\n",
      "  )\n",
      "  (bn1): BatchNorm1d(1327, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear1): Linear(in_features=1327, out_features=500, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dp1): Dropout(p=0.35, inplace=False)\n",
      "  (linear2): Linear(in_features=500, out_features=500, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (linear3): Linear(in_features=500, out_features=78, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models_sherlock import FeatureEncoder, SherlockClassifier, build_sherlock\n",
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "sherlock_feature_groups = ['char', 'rest', 'par', 'word']\n",
    "\n",
    "\n",
    "classifier = build_sherlock(sherlock_feature_groups, num_classes=78, topic_dim=400, dropout_ratio=0.35).to(device)\n",
    "\n",
    "model_loc = join(os.environ['BASEPATH'],\n",
    "                         'model', 'pre_trained_sherlock', \"type78\")\n",
    "pretrained_shelock_path = f\"all_None.pt\"\n",
    "\n",
    "classifier.load_state_dict(torch.load(join(model_loc,pretrained_shelock_path), map_location=device))\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(classifier.linear3.in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=500, out_features=255, bias=True)\n",
      "tensor([[-0.0928,  0.0366,  0.0606,  ...,  0.0746,  0.0451,  0.0076],\n",
      "        [-0.0411, -0.0170,  0.0441,  ..., -0.0745, -0.0485, -0.0397],\n",
      "        [ 0.0325, -0.0013, -0.0041,  ...,  0.0792,  0.0886, -0.0688],\n",
      "        ...,\n",
      "        [-0.0868, -0.0144,  0.0711,  ..., -0.0072, -0.1660, -0.0214],\n",
      "        [ 0.0217,  0.0364, -0.0823,  ..., -0.0953, -0.0105, -0.0732],\n",
      "        [ 0.0558, -0.0606,  0.0925,  ..., -0.0960, -0.0182, -0.0405]])\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "classifier.linear3.out_features = 255\n",
    "print(classifier.linear3)\n",
    "print(classifier.linear3.weight.data)\n",
    "print(len(classifier.linear3.weight.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7c4e269c0269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "classifier.linear3 = nn.Linear(500, 255)\n",
    "print(classifier.linear3.weight.data)\n",
    "print(len(classifier.linear3.weight.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain sherlock and check the las layer \n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# set env-var\n",
    "os.environ['BASEPATH'] = 'D:\\\\20120321_DHBW_AZUREML\\\\sato'\n",
    "os.environ['RAW_DIR'] = '/mnt/batch/tasks/shared/LS_root/mounts/clusters/dhbwmlc-ds12-v2/code/Users/svenlangenecker/viznet-master/raw' # path to the raw data\n",
    "os.environ['SHERLOCKPATH'] = os.environ['BASEPATH']+'\\\\sherlock'\n",
    "os.environ['EXTRACTPATH'] = os.environ['BASEPATH']+'\\\\extract'\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['SHERLOCKPATH']\n",
    "#os.environ['PYTHONPATH'] = os.environ['PYTHONPATH']+':'+os.environ['BASEPATH']\n",
    "os.environ['TYPENAME'] = 'type78'\n",
    "\n",
    "percent = 50\n",
    "# if index != 0:\n",
    "#     continue\n",
    "comment = f\"labeled{percent}_unlabeled{100-20-percent}_test{20}_255types\"\n",
    "column_level_split_file_path = f\"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_{percent}_{100-20-percent}_20.json\"\n",
    "pretrained_shelock_path = f\"sherlock_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}_255types.pt\"\n",
    "pretrained_CRF_LDA_path = f\"CRF+LDA_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}.pt\"\n",
    "\n",
    "# retrain sherlock\n",
    "%run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_retrain.txt  --comment {comment} --column_level_split_file_path {column_level_split_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_shelock_path = f\"sherlock_retrain_labeled{percent}_unlabeled{100-20-percent}_test{20}_255types.pt\"\n",
    "\n",
    "%run ../model/train_sherlock.py -c ../model/params/publicbi/sherlock_eval.txt --model_list {pretrained_shelock_path} --comment \"sherlock_baseline_column_level_split_test20\" --column_level_split_file_path \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\extract\\\\out\\\\labeled_unlabeled_test_split\\\\public_bi_10_70_20.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import classification_report\n",
    "from torchcrf import CRF\n",
    "\n",
    "model = CRF(len(get_valid_types(os.environ[\"TYPENAME\"])), batch_first=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0239, -0.0149, -0.0826, -0.0511, -0.0959, -0.0794, -0.0002,  0.0272,\n",
       "        -0.0487, -0.0631,  0.0808,  0.0082,  0.0215, -0.0863, -0.0891,  0.0871,\n",
       "        -0.0959,  0.0727, -0.0038,  0.0935,  0.0261, -0.0030,  0.0183, -0.0093,\n",
       "         0.0455,  0.0823,  0.0944, -0.0145,  0.0942,  0.0691, -0.0679, -0.0374,\n",
       "        -0.0873, -0.0450, -0.0352,  0.0315,  0.0378, -0.0441, -0.0358, -0.0855,\n",
       "        -0.0642, -0.0608, -0.0667, -0.0037,  0.0939, -0.0894, -0.0773,  0.0999,\n",
       "         0.0339, -0.0458, -0.0915, -0.0563,  0.0210, -0.0369,  0.0050, -0.0677,\n",
       "         0.0302, -0.0014, -0.0966, -0.0502,  0.0218, -0.0035,  0.0945, -0.0549,\n",
       "        -0.0343,  0.0696, -0.0005, -0.0310,  0.0870, -0.0132,  0.0915,  0.0084,\n",
       "        -0.0884,  0.0759,  0.0126,  0.0129,  0.0783,  0.0970])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.start_transitions.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0980, -0.0819, -0.0186,  ...,  0.0923, -0.0348,  0.0196],\n",
       "        [ 0.0367, -0.0308, -0.0216,  ...,  0.0458, -0.0933,  0.0820],\n",
       "        [-0.0808, -0.0673,  0.0429,  ...,  0.0475, -0.0833,  0.0241],\n",
       "        ...,\n",
       "        [ 0.0523,  0.0672, -0.0991,  ...,  0.0986, -0.0761, -0.0089],\n",
       "        [ 0.0313,  0.0764,  0.0594,  ...,  0.0779,  0.0865,  0.0542],\n",
       "        [ 0.0740,  0.0074, -0.0683,  ...,  0.0216,  0.0247,  0.0993]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transitions.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat' 'dog']\n"
     ]
    }
   ],
   "source": [
    "test_labels = [\"cat\", \"dog\"]\n",
    "label_enc_test = LabelEncoder()\n",
    "label_enc_test.fit(test_labels)\n",
    "\n",
    "print(label_enc_test.inverse_transform([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'bird', 'zebra', 'aligator']\n",
      "['aligator' 'bird' 'cat' 'dog' 'zebra']\n"
     ]
    }
   ],
   "source": [
    "test_labels.append(\"bird\")\n",
    "test_labels.append(\"zebra\")\n",
    "test_labels.append(\"aligator\")\n",
    "print(test_labels)\n",
    "\n",
    "label_enc_test = LabelEncoder()\n",
    "label_enc_test.fit(test_labels)\n",
    "\n",
    "print(label_enc_test.inverse_transform([0,1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aligator', 'bird', 'cat', 'dog', 'zebra'], dtype='<U8')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_enc_test.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b27a53400a11b77b02f1d2309c8bc5e534ca1e422654b13082ae5af4dd3feced"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('sato': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
