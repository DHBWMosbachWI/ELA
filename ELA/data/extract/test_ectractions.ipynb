{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "os.environ[\"WORKING_DIR\"] = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\"\n",
    "os.environ[\"TYPENAME\"] = \"type_gittables_abstraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_types_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\", \"valid_types\", \"types.json\")\n",
    "# load array of valid types\n",
    "with open(valid_types_path, \"r\") as file:\n",
    "    valid_types = json.load(file)[os.environ[\"TYPENAME\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(valid_types))\n",
    "# build a Label Encoder \n",
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(valid_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc.inverse_transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[\"test\", \"[1,2,3]\"]], columns=[\"0\", \"1\"])\n",
    "df[\"1\"] = df[\"1\"].apply(eval)\n",
    "def label_enc_f(x):\n",
    "    return label_enc.inverse_transform(x)\n",
    "df[\"1\"] = df[\"1\"].apply(label_enc_f)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_split_labeled_unlabeled_test_absolut.py --valid_headers gittables-abstraction-tables_type_gittables_abstraction_header_valid.csv --corpus gittables-abstraction-tables --labeled_size 1 --test_size 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TYPENAME\"] = \"type_turl\"\n",
    "\n",
    "for value in np.arange(1,6,1):\n",
    "    for random_state in [1,2,3,4,5]:\n",
    "        %run data_split_labeled_unlabeled_test_absolut.py --valid_headers turl_type_turl_header_valid.csv --corpus turl --labeled_size {value} --test_size 0.2 --random_state {random_state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002\n",
      "Labeled data: 1048(0.00), Unlabeled data: 418272(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 1048(0.00), Unlabeled data: 418272(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 1048(0.00), Unlabeled data: 418272(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 1048(0.00), Unlabeled data: 418272(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 1048(0.00), Unlabeled data: 418272(0.80), Test data: 104831(0.20)\n",
      "0.004\n",
      "Labeled data: 2096(0.00), Unlabeled data: 417224(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 2096(0.00), Unlabeled data: 417224(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 2096(0.00), Unlabeled data: 417224(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 2096(0.00), Unlabeled data: 417224(0.80), Test data: 104831(0.20)\n",
      "Labeled data: 2096(0.00), Unlabeled data: 417224(0.80), Test data: 104831(0.20)\n",
      "0.006\n",
      "Labeled data: 3144(0.01), Unlabeled data: 416176(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 3144(0.01), Unlabeled data: 416176(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 3144(0.01), Unlabeled data: 416176(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 3144(0.01), Unlabeled data: 416176(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 3144(0.01), Unlabeled data: 416176(0.79), Test data: 104831(0.20)\n",
      "0.008\n",
      "Labeled data: 4193(0.01), Unlabeled data: 415127(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 4193(0.01), Unlabeled data: 415127(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 4193(0.01), Unlabeled data: 415127(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 4193(0.01), Unlabeled data: 415127(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 4193(0.01), Unlabeled data: 415127(0.79), Test data: 104831(0.20)\n",
      "0.01\n",
      "Labeled data: 5241(0.01), Unlabeled data: 414079(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 5241(0.01), Unlabeled data: 414079(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 5241(0.01), Unlabeled data: 414079(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 5241(0.01), Unlabeled data: 414079(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 5241(0.01), Unlabeled data: 414079(0.79), Test data: 104831(0.20)\n",
      "0.012\n",
      "Labeled data: 6289(0.01), Unlabeled data: 413031(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 6289(0.01), Unlabeled data: 413031(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 6289(0.01), Unlabeled data: 413031(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 6289(0.01), Unlabeled data: 413031(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 6289(0.01), Unlabeled data: 413031(0.79), Test data: 104831(0.20)\n",
      "0.014\n",
      "Labeled data: 7338(0.01), Unlabeled data: 411982(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 7338(0.01), Unlabeled data: 411982(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 7338(0.01), Unlabeled data: 411982(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 7338(0.01), Unlabeled data: 411982(0.79), Test data: 104831(0.20)\n",
      "Labeled data: 7338(0.01), Unlabeled data: 411982(0.79), Test data: 104831(0.20)\n",
      "0.016\n",
      "Labeled data: 8386(0.02), Unlabeled data: 410934(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 8386(0.02), Unlabeled data: 410934(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 8386(0.02), Unlabeled data: 410934(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 8386(0.02), Unlabeled data: 410934(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 8386(0.02), Unlabeled data: 410934(0.78), Test data: 104831(0.20)\n",
      "0.018\n",
      "Labeled data: 9434(0.02), Unlabeled data: 409886(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 9434(0.02), Unlabeled data: 409886(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 9434(0.02), Unlabeled data: 409886(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 9434(0.02), Unlabeled data: 409886(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 9434(0.02), Unlabeled data: 409886(0.78), Test data: 104831(0.20)\n",
      "0.02\n",
      "Labeled data: 10483(0.02), Unlabeled data: 408837(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 10483(0.02), Unlabeled data: 408837(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 10483(0.02), Unlabeled data: 408837(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 10483(0.02), Unlabeled data: 408837(0.78), Test data: 104831(0.20)\n",
      "Labeled data: 10483(0.02), Unlabeled data: 408837(0.78), Test data: 104831(0.20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"TYPENAME\"] = \"type_turl\"\n",
    "\n",
    "for value in np.around(np.arange(0.002,0.022,0.002),3):\n",
    "    print(value)\n",
    "    for random_state in [1,2,3,4,5]:\n",
    "        %run data_split_labeled_unlabeled_test.py --valid_headers turl_type_turl_header_valid.csv --corpus turl --labeled_size {value} --unlabeled_size {0.80-value} --test_size 0.2 --random_state {random_state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform sato csv valid header format into embclus json valid header format\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"./out/valid_types/types.json\", \"r\") as f:\n",
    "    valid_types = json.load(f)\n",
    "    valid_types = valid_types[\"type_turl\"]\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "label_enc.fit(valid_types)\n",
    "\n",
    "#label_enc.inverse_transform([0])\n",
    "\n",
    "dic = {}\n",
    "df = pd.read_csv(\"./out/valid_headers/turl_type_turl_header_valid.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    dataset_id = row[\"dataset_id\"]\n",
    "    field_list = eval(row[\"field_list\"])\n",
    "    field_names = eval(row[\"field_names\"])\n",
    "    dic[dataset_id] = {}\n",
    "    for index_2, column in enumerate(field_list):\n",
    "        dic[dataset_id][f\"column_{column}\"] = {\n",
    "            \"semanticType\": label_enc.inverse_transform([field_names[index_2]])[0]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./out/valid_headers/turl_type_turl_header_valid.json\", \"w\") as f:\n",
    "    json.dump(dic,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot stats of splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "from multiprocesspandas import applyparallel\n",
    "\n",
    "os.environ[\"WORKING_DIR\"] = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\"\n",
    "\n",
    "valid_header_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\",\n",
    "                         \"valid_headers\")\n",
    "\n",
    "labeled = 1.0\n",
    "test = 20.0\n",
    "unlabeled = 100-test-labeled\n",
    "#unlabeled = \"absolute\"\n",
    "\n",
    "valid_headers_file = f\"turl_type_turl_valid.json\"\n",
    "\n",
    "# load the valid headers with read sem. types\n",
    "valid_headers = join(valid_header_path, valid_headers_file)\n",
    "\n",
    "\n",
    "with open(valid_headers, \"r\") as file:\n",
    "    valid_headers = json.load(file)\n",
    "\n",
    "# transform valid header into df to make splitable\n",
    "valid_header_df_data = []\n",
    "for table in valid_headers.keys():\n",
    "    for column in valid_headers[table].keys():\n",
    "        valid_header_df_data.append(\n",
    "            [table, column, valid_headers[table][column][\"semanticType\"]])\n",
    "valid_header_df = pd.DataFrame(valid_header_df_data,\n",
    "                            columns=[\"table\", \"column\", \"semanticType\"])\n",
    "\n",
    "valid_header_df[\"dataset_id\"] = valid_header_df[\"table\"]+\"+\"+valid_header_df[\"column\"]\n",
    "print(valid_header_df.head())\n",
    "\n",
    "\n",
    "#valid_header_df = valid_header_df.iloc[:500]\n",
    "\n",
    "# load laebeled_unlabeled_test_split\n",
    "split_file_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\", \"labeled_unlabeled_test_split\")\n",
    "split_file = f\"turl_{labeled}_{unlabeled}_{test}.json\"\n",
    "\n",
    "with open(join(split_file_path, split_file), \"r\") as file:\n",
    "    split_file = json.load(file)\n",
    "\n",
    "df_split_labeled = pd.DataFrame({\"dataset_id\": split_file[f\"labeled{labeled}\"]})\n",
    "df_split_labeled[\"split_part\"] = \"labeled\"\n",
    "df_split_unlabeled = pd.DataFrame({\"dataset_id\": split_file[f\"unlabeled{unlabeled}\"]})\n",
    "#df_split_unlabeled = pd.DataFrame({\"dataset_id\": split_file[f\"unlabeled\"]})\n",
    "df_split_unlabeled[\"split_part\"] = \"unlabeled\"\n",
    "df_split_test = pd.DataFrame({\"dataset_id\": split_file[f\"test{test}\"]})\n",
    "df_split_test[\"split_part\"] = \"test\"\n",
    "\n",
    "df_split = pd.concat([df_split_labeled, df_split_unlabeled, df_split_test])\n",
    "\n",
    "#df_split.head(-20)\n",
    "\n",
    "df = valid_header_df.join(df_split.set_index(\"dataset_id\"), on=\"dataset_id\")\n",
    "\n",
    "save_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\", \"labeled_unlabeled_test_split\")\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "\n",
    "ax = df[df[\"split_part\"] == \"labeled\"].groupby([\"semanticType\"]).size().sort_values().plot.barh()\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"#semantic types of labaled data ({labeled}/{unlabeled}/{test})\")\n",
    "plt.savefig(join(save_path, f\"turl_{labeled}_{unlabeled}_{test}_stats_labeled.png\"), bbox_inches=\"tight\", facecolor=\"white\", transparent=False)\n",
    "#plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "\n",
    "ax = df[df[\"split_part\"] == \"unlabeled\"].groupby([\"semanticType\"]).size().sort_values().plot.barh()\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"#semantic types of unlabaled data ({labeled}/{unlabeled}/{test})\")\n",
    "plt.savefig(join(save_path, f\"turl_{labeled}_{unlabeled}_{test}_stats_unlabeled.png\"), bbox_inches=\"tight\", facecolor=\"white\", transparent=False)\n",
    "#plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "\n",
    "ax = df[df[\"split_part\"] == \"test\"].groupby([\"semanticType\"]).size().sort_values().plot.barh()\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"#semantic types of test data ({labeled}/{unlabeled}/{test})\")\n",
    "plt.savefig(join(save_path, f\"turl_{labeled}_{unlabeled}_{test}_stats_test.png\"), bbox_inches=\"tight\", facecolor=\"white\", transparent=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(valid_header_df[\"semanticType\"].tolist(),return_counts=True)\n",
    "pd.DataFrame({\"lables\":labels, \"counts\":counts}).sort_values(by=\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\", \"labeled_unlabeled_test_split\")\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "\n",
    "ax = df[df[\"split_part\"] == \"labeled\"].groupby([\"semanticType\"]).size().sort_values().plot.barh()\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"#semantic types of labaled data ({labeled}/{unlabeled}/{test})\")\n",
    "plt.savefig(join(save_path, f\"turl_{labeled}_{unlabeled}_{test}_stats_labeled.png\"), bbox_inches=\"tight\", facecolor=\"white\", transparent=False)\n",
    "#plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "\n",
    "ax = df[df[\"split_part\"] == \"unlabeled\"].groupby([\"semanticType\"]).size().sort_values().plot.barh()\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"#semantic types of unlabaled data ({labeled}/{unlabeled}/{test})\")\n",
    "plt.savefig(join(save_path, f\"turl_{labeled}_{unlabeled}_{test}_stats_unlabeled.png\"), bbox_inches=\"tight\", facecolor=\"white\", transparent=False)\n",
    "#plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,30))\n",
    "\n",
    "ax = df[df[\"split_part\"] == \"test\"].groupby([\"semanticType\"]).size().sort_values().plot.barh()\n",
    "\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container)\n",
    "\n",
    "plt.grid()\n",
    "plt.title(f\"#semantic types of test data ({labeled}/{unlabeled}/{test})\")\n",
    "plt.savefig(join(save_path, f\"turl_{labeled}_{unlabeled}_{test}_stats_test.png\"), bbox_inches=\"tight\", facecolor=\"white\", transparent=False)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for labeled in np.around(np.arange(0.1,1,0.1),1):\n",
    "    %run plot_stats_labeled_unlabeled_test_split.py --labeled {labeled}  --unlabeled {80-labeled} --test 20 --corpus \"turl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "import os\n",
    "\n",
    "os.environ[\"WORKING_DIR\"] = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\"\n",
    "\n",
    "# valid_header_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\",\n",
    "#                          \"valid_headers\", \"turl_time.event\")\n",
    "# df_valid_header = pd.read_csv(join(valid_header_path, f\"turl_type_turl_header_valid.csv\"))\n",
    "\n",
    "valid_header_path = join(os.environ[\"WORKING_DIR\"], \"data\", \"extract\", \"out\",\n",
    "                         \"valid_headers\")\n",
    "df_valid_header = pd.read_csv(join(valid_header_path, f\"public_bi_type78.csv\"))\n",
    "\n",
    "headers = []\n",
    "column_numbers = []\n",
    "for index, row in df_valid_header.iterrows():\n",
    "    headers.extend(eval(row[\"field_names\"]))\n",
    "    column_numbers.append(len(eval(row[\"field_names\"])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts  = np.unique(headers, return_counts=True)\n",
    "pd.DataFrame({\"labels\":label_enc.inverse_transform(labels),\"counts\":counts }).sort_values(by=\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.955974842767295"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "statistics.mean(column_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(column_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
