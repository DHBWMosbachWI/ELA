{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PUBLIC_BI_BENCHMARK\"] = \"D:\\\\semantic_data_lake\\\\semantic_data_lake\\\\data\\\\benchmark\\\\\"\n",
    "import sys\n",
    "sys.path.insert(0, \"D:\\\\semantic_data_lake\\\\semantic_data_lake\")\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType, StructType, StructField\n",
    "from pyspark.sql.functions import udf, col, pandas_udf, PandasUDFType, collect_list, count, avg, lit, mean, stddev, monotonically_increasing_id, row_number\n",
    "from scipy.stats import wasserstein_distance\n",
    "from numpy import asarray\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import permutations, combinations, combinations_with_replacement\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from helper_functions import print_df_to_html, translate_header_file_to_list, variations, pair_permutations_ordered, translate_datatype_file_to_list, cast_datatypes, check_attribute_completeness, compare_schemas\n",
    "from labeling_functions.similarity_calculations import appr3_calc_similarities, appr4_calc_similarities\n",
    "from data_loader.adding_noise import add_additive_gaussian_noise\n",
    "from data_loader.utils import load_public_bi_table\n",
    "from data_loader.utils_for_pyspark import load_public_bi_tables_in_spark_temp_view\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report for labeling performance\n",
    "def classification_report_labeling(df,cols_to_label, max_group_count:int):\n",
    "    #col_to_label = \"H\"\n",
    "    #max_group_count = 1\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\n",
    "    for column in cols_to_label:\n",
    "        if df.select(\"*\").where(col(\"INNER_ATTR\") == column).count() == 0:\n",
    "            continue\n",
    "        pred_label = df.select(\"*\").where(col(\"INNER_ATTR\") == column).where(col(\"count\") <= max_group_count).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").collect()[0][\"OUTER_ATTR\"]\n",
    "        #print(pred_label)\n",
    "        true_labels.append(column)\n",
    "        pred_labels.append(pred_label)\n",
    "    class_report_dic = classification_report(true_labels, pred_labels, output_dict=True)\n",
    "    return class_report_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sprak Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sparkContext.stop()\n",
    "# create Spark Config\n",
    "conf = SparkConf()\n",
    "#conf.set(\"spark.executor.instances\",\"2\")\n",
    "#conf.set(\"spark.executor.cores\",\"2\")\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\n",
    "conf.set(\"spark.driver.memory\", \"15g\")\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "conf.set(\"spark.memory.offHeap.size\", \"16g\")\n",
    "#conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"MLB-similarity-calc\")\n",
    "# create a SparkSession\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.emd_UDF(col1, col2) -> pyspark.sql.types.FloatType>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and register UDF-Function to calc EMD-Distance\n",
    "@udf(returnType=FloatType())\n",
    "def emd_UDF(col1, col2) -> FloatType:\n",
    "    return float(wasserstein_distance(col1, col2))\n",
    "\n",
    "spark.udf.register(\"emd_UDF\", emd_UDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the tables into spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation_40532458112880653 is not numeric or string\n",
      "Calculation_40532458117070874 is not numeric or string\n",
      "Calculation_40532458117263387 is not numeric or string\n",
      "Calculation_40532458113208334 is not numeric or string\n",
      "Calculation_40532458117263387 is not numeric or string\n",
      "Calculation_40532458113208334 is not numeric or string\n",
      "Calculation_40532458112880653 is not numeric or string\n",
      "Calculation_40532458117070874 is not numeric or string\n",
      "Calculation_40532458112880653 is not numeric or string\n",
      "Calculation_40532458117070874 is not numeric or string\n",
      "Calculation_40532458112880653 is not numeric or string\n",
      "Calculation_40532458117070874 is not numeric or string\n",
      "Calculation_496521908233621517 is not numeric or string\n",
      "Calculation_40532458112270348 is not numeric or string\n",
      "Calculation_40532458116673561 is not numeric or string\n",
      "Calculation_496521908233621517 is not numeric or string\n",
      "Calculation_40532458112270348 is not numeric or string\n",
      "Calculation_40532458116673561 is not numeric or string\n",
      "Calculation_496521908233621517 is not numeric or string\n",
      "Calculation_40532458112270348 is not numeric or string\n",
      "Calculation_40532458116673561 is not numeric or string\n",
      "Calculation_496521908233621517 is not numeric or string\n",
      "Calculation_40532458112270348 is not numeric or string\n",
      "Calculation_40532458116673561 is not numeric or string\n",
      "Calculation_361765767527686145 is not numeric or string\n",
      "Calculation_40532458111905803 is not numeric or string\n",
      "Calculation_40532458116419608 is not numeric or string\n",
      "Calculation_40532458064297984 is not numeric or string\n",
      "Calculation_361765767527686145 is not numeric or string\n",
      "Calculation_40532458111905803 is not numeric or string\n",
      "Calculation_40532458116419608 is not numeric or string\n",
      "Calculation_361765767527686145 is not numeric or string\n",
      "Calculation_40532458111905803 is not numeric or string\n",
      "Calculation_40532458116419608 is not numeric or string\n",
      "Calculation_496521908233404428 is not numeric or string\n",
      "Calculation_40532458111283210 is not numeric or string\n",
      "Calculation_40532458116173847 is not numeric or string\n",
      "Calculation_496521908233404428 is not numeric or string\n",
      "Calculation_40532458111283210 is not numeric or string\n",
      "Calculation_40532458116173847 is not numeric or string\n",
      "Calculation_496521908233404428 is not numeric or string\n",
      "Calculation_40532458111283210 is not numeric or string\n",
      "Calculation_40532458116173847 is not numeric or string\n",
      "Calculation_496521908233248779 is not numeric or string\n",
      "Calculation_40532458110861321 is not numeric or string\n",
      "Calculation_40532458115964950 is not numeric or string\n",
      "Calculation_496521908233248779 is not numeric or string\n",
      "Calculation_40532458110861321 is not numeric or string\n",
      "Calculation_40532458115964950 is not numeric or string\n",
      "Calculation_496521908233248779 is not numeric or string\n",
      "Calculation_40532458110861321 is not numeric or string\n",
      "Calculation_40532458115964950 is not numeric or string\n",
      "Calculation_496521908233248779 is not numeric or string\n",
      "Calculation_40532458110861321 is not numeric or string\n",
      "Calculation_40532458115964950 is not numeric or string\n",
      "Calculation_496521908233248779 is not numeric or string\n",
      "Calculation_40532458110861321 is not numeric or string\n",
      "Calculation_40532458115964950 is not numeric or string\n",
      "Calculation_40532458064297984 is not numeric or string\n",
      "Calculation_496521908233248779 is not numeric or string\n",
      "Calculation_40532458110861321 is not numeric or string\n",
      "Calculation_40532458115964950 is not numeric or string\n",
      "Calculation_361765767503851520 is not numeric or string\n",
      "Calculation_40532458110562312 is not numeric or string\n",
      "Calculation_40532458115731477 is not numeric or string\n",
      "Calculation_361765767503851520 is not numeric or string\n",
      "Calculation_40532458110562312 is not numeric or string\n",
      "Calculation_40532458115731477 is not numeric or string\n",
      "Calculation_361765767503851520 is not numeric or string\n",
      "Calculation_40532458110562312 is not numeric or string\n",
      "Calculation_40532458115731477 is not numeric or string\n",
      "Calculation_361765767503851520 is not numeric or string\n",
      "Calculation_40532458110562312 is not numeric or string\n",
      "Calculation_40532458115731477 is not numeric or string\n",
      "Calculation_361765767503851520 is not numeric or string\n",
      "Calculation_40532458110562312 is not numeric or string\n",
      "Calculation_40532458115731477 is not numeric or string\n",
      "Calculation_361765767503851520 is not numeric or string\n",
      "Calculation_40532458110562312 is not numeric or string\n",
      "Calculation_40532458115731477 is not numeric or string\n",
      "Calculation_361765767503851520 is not numeric or string\n",
      "Calculation_40532458110562312 is not numeric or string\n",
      "Calculation_40532458115731477 is not numeric or string\n",
      "Calculation_496521908232941578 is not numeric or string\n",
      "Calculation_40532458110046215 is not numeric or string\n",
      "Calculation_40532458115493908 is not numeric or string\n",
      "Calculation_496521908232941578 is not numeric or string\n",
      "Calculation_40532458110046215 is not numeric or string\n",
      "Calculation_40532458115493908 is not numeric or string\n",
      "Calculation_40532458064297984 is not numeric or string\n",
      "Calculation_496521908232941578 is not numeric or string\n",
      "Calculation_40532458110046215 is not numeric or string\n",
      "Calculation_40532458115493908 is not numeric or string\n",
      "Calculation_496521908230012937 is not numeric or string\n",
      "Calculation_40532458109407238 is not numeric or string\n",
      "Calculation_40532458115264531 is not numeric or string\n",
      "Calculation_496521908230012937 is not numeric or string\n",
      "Calculation_40532458109407238 is not numeric or string\n",
      "Calculation_40532458115264531 is not numeric or string\n",
      "Calculation_496521908230012937 is not numeric or string\n",
      "Calculation_40532458109407238 is not numeric or string\n",
      "Calculation_40532458115264531 is not numeric or string\n",
      "Calculation_496521908230012937 is not numeric or string\n",
      "Calculation_40532458109407238 is not numeric or string\n",
      "Calculation_40532458115264531 is not numeric or string\n",
      "Calculation_496521908229181448 is not numeric or string\n",
      "Calculation_40532458108895237 is not numeric or string\n",
      "Calculation_40532458114994194 is not numeric or string\n",
      "Calculation_496521908229181448 is not numeric or string\n",
      "Calculation_40532458108895237 is not numeric or string\n",
      "Calculation_40532458114994194 is not numeric or string\n",
      "Calculation_496521908229181448 is not numeric or string\n",
      "Calculation_40532458108895237 is not numeric or string\n",
      "Calculation_40532458114994194 is not numeric or string\n",
      "Calculation_496521908229181448 is not numeric or string\n",
      "Calculation_40532458108895237 is not numeric or string\n",
      "Calculation_40532458114994194 is not numeric or string\n",
      "Calculation_496521908228587527 is not numeric or string\n",
      "Calculation_40532458108489732 is not numeric or string\n",
      "Calculation_40532458114363409 is not numeric or string\n",
      "Calculation_361765767559729157 is not numeric or string\n",
      "Calculation_496521908228587527 is not numeric or string\n",
      "Calculation_40532458108489732 is not numeric or string\n",
      "Calculation_40532458114363409 is not numeric or string\n",
      "Calculation_496521908228587527 is not numeric or string\n",
      "Calculation_40532458108489732 is not numeric or string\n",
      "Calculation_40532458114363409 is not numeric or string\n",
      "Calculation_496521908228263942 is not numeric or string\n",
      "Calculation_212021079764389888 is not numeric or string\n",
      "Calculation_40532458107953155 is not numeric or string\n",
      "Calculation_40532458114015248 is not numeric or string\n",
      "Calculation_496521908228263942 is not numeric or string\n",
      "Calculation_212021079764389888 is not numeric or string\n",
      "Calculation_40532458107953155 is not numeric or string\n",
      "Calculation_40532458114015248 is not numeric or string\n",
      "Calculation_496521908228263942 is not numeric or string\n",
      "Calculation_212021079764389888 is not numeric or string\n",
      "Calculation_40532458107953155 is not numeric or string\n",
      "Calculation_40532458114015248 is not numeric or string\n",
      "Calculation_496521908228263942 is not numeric or string\n",
      "Calculation_212021079764389888 is not numeric or string\n",
      "Calculation_40532458107953155 is not numeric or string\n",
      "Calculation_40532458114015248 is not numeric or string\n",
      "Calculation_496521907637829636 is not numeric or string\n",
      "Calculation_212021079765098497 is not numeric or string\n",
      "Calculation_40532458107371522 is not numeric or string\n",
      "Calculation_40532458113540111 is not numeric or string\n",
      "Calculation_496521907637829636 is not numeric or string\n",
      "Calculation_212021079765098497 is not numeric or string\n",
      "Calculation_40532458107371522 is not numeric or string\n",
      "Calculation_40532458113540111 is not numeric or string\n",
      "Calculation_361765767559729157 is not numeric or string\n",
      "Calculation_496521907637829636 is not numeric or string\n",
      "Calculation_212021079765098497 is not numeric or string\n",
      "Calculation_40532458107371522 is not numeric or string\n",
      "Calculation_40532458113540111 is not numeric or string\n",
      "Calculation_496521907637829636 is not numeric or string\n",
      "Calculation_212021079765098497 is not numeric or string\n",
      "Calculation_40532458107371522 is not numeric or string\n",
      "Calculation_40532458113540111 is not numeric or string\n",
      "Calculation_496521907637829636 is not numeric or string\n",
      "Calculation_212021079765098497 is not numeric or string\n",
      "Calculation_40532458107371522 is not numeric or string\n",
      "Calculation_40532458113540111 is not numeric or string\n",
      "Calculation_496521907638001669 is not numeric or string\n",
      "Calculation_496521907631644673 is not numeric or string\n",
      "Calculation_496521907632136194 is not numeric or string\n",
      "Calculation_496521907638001669 is not numeric or string\n",
      "Calculation_496521907631644673 is not numeric or string\n",
      "Calculation_496521907632136194 is not numeric or string\n",
      "Calculation_496521907638001669 is not numeric or string\n",
      "Calculation_496521907631644673 is not numeric or string\n",
      "Calculation_496521907632136194 is not numeric or string\n",
      "Calculation_40532458064297984 is not numeric or string\n",
      "Calculation_361765767559729157 is not numeric or string\n",
      "Calculation_40532458117263387 is not numeric or string\n",
      "Calculation_40532458113208334 is not numeric or string\n",
      "Calculation_40532458117263387 is not numeric or string\n",
      "Calculation_40532458113208334 is not numeric or string\n",
      "Calculation_40532458117263387 is not numeric or string\n",
      "Calculation_40532458113208334 is not numeric or string\n"
     ]
    }
   ],
   "source": [
    "string_attributes, numeric_attributes = load_public_bi_tables_in_spark_temp_view(\"MLB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+---+---+---+---+\n",
      "|       batter_name|parentteam| hr|x1b|x2b|x3b|\n",
      "+------------------+----------+---+---+---+---+\n",
      "|     Chase Larsson|       ATL|  1|  7|  6|  0|\n",
      "|     Kenny Fleming|       ATL|  0|  7|  0|  0|\n",
      "|      Kurt Fleming|       ATL|  0| 13|  2|  1|\n",
      "|      Kyle Kubitza|       ATL|  0| 11|  3|  3|\n",
      "|   Nick DeSantiago|       ATL|  0|  5|  2|  0|\n",
      "|      Adam Heisler|       CWS|  0|  2|  0|  0|\n",
      "|      Brent Tanner|       CWS|  0|  3|  0|  0|\n",
      "|    Carlos Sanchez|       CWS|  0|  1|  1|  0|\n",
      "|       Cory Farris|       CWS|  1|  7|  1|  1|\n",
      "|     Drew Thompson|       CWS|  0|  0|  0|  0|\n",
      "|     Dusty Harvard|       CWS|  0|  5|  2|  1|\n",
      "|     Jordan Keegan|       CWS|  0|  9|  0|  0|\n",
      "|   Michael Johnson|       CWS|  0| 13|  2|  1|\n",
      "|  Michael Schwartz|       CWS|  0|  3|  2|  0|\n",
      "|    Sean O'Connell|       CWS|  0|  3|  1|  0|\n",
      "|      Ariel Ovando|       HOU|  0|  8|  3|  2|\n",
      "|         Chan Moon|       HOU|  0|  7|  1|  1|\n",
      "|    Chase Davidson|       HOU|  4|  9|  4|  0|\n",
      "|      Jean Batista|       HOU|  0|  2|  2|  0|\n",
      "|      Jordan Scott|       HOU|  0| 23|  5|  3|\n",
      "| Kellen Kiilsgaard|       HOU|  0|  0|  0|  0|\n",
      "|    Ronald Sanchez|       HOU|  0|  2|  0|  0|\n",
      "|        Ruben Sosa|       HOU|  0|  3|  1|  0|\n",
      "|    Anthony Howard|        KC|  1|  3|  0|  0|\n",
      "|    Cameron Conner|        KC|  0|  7|  1|  1|\n",
      "|    Jose Rodriguez|        KC|  0|  5|  1|  1|\n",
      "|     Jovan Pickett|        KC|  0|  6|  0|  0|\n",
      "|      Murray Watts|        KC|  4|  7|  3|  0|\n",
      "|   Yunior Figueroa|        KC|  0|  0|  0|  0|\n",
      "|     Eddie Rosario|       MIN|  6| 24|  2|  3|\n",
      "| JaDamion Williams|       MIN|  0|  6|  0|  0|\n",
      "|     Kennys Vargas|       MIN|  1| 11|  2|  0|\n",
      "|        Max Kepler|       MIN|  0|  8|  7|  0|\n",
      "|      Niko Goodrum|       MIN|  0| 17|  1|  2|\n",
      "|      Camden Maron|       NYM|  0| 11|  3|  1|\n",
      "|      Carlos Leyva|       NYM|  0|  5|  0|  0|\n",
      "|    Jonathan Clark|       NYM|  0|  4|  1|  1|\n",
      "|Juan Carlos Gamboa|       NYM|  0|  3|  2|  0|\n",
      "|     Lucas Stewart|       NYM|  0|  5|  2|  1|\n",
      "|     Nestor Moreno|       NYM|  0|  7|  0|  0|\n",
      "|   Alfredo Morales|       SEA|  0|  9|  0|  0|\n",
      "|      Efrain Nunez|       SEA|  4|  3|  1|  1|\n",
      "| Frankie Christian|       SEA|  0|  3|  0|  0|\n",
      "|Guillermo Pimentel|       SEA|  2| 13|  5|  0|\n",
      "|    Kenneth Straus|       SEA|  1|  4|  0|  0|\n",
      "|    Breyvil Valera|       STL|  0|  8|  2|  0|\n",
      "|    Charlie Tilson|       STL|  0|  1|  0|  0|\n",
      "|  David Washington|       STL|  1|  9|  2|  1|\n",
      "|        Kolby Byrd|       STL|  0|  8|  3|  0|\n",
      "|   Cameron Seitzer|        TB|  2| 15|  6|  0|\n",
      "+------------------+----------+---+---+---+---+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT batter_name, parentteam, hr, x1b, x2b, x3b FROM MLB_1\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise with Approach 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPROACH 3\n",
    "# duration of about 7 hours\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "table_name = \"MLB_1\"\n",
    "data_perc_with_noise = 0.1\n",
    "max_group_count = 3\n",
    "\n",
    "df_noise = add_additive_gaussian_noise(\n",
    "    table_name, data_perc_with_noise, random_seed, numeric_attributes)\n",
    "\n",
    "numeric_attributes[table_name+\"_noise\"] = numeric_attributes[table_name]\n",
    "string_attributes[table_name+\"_noise\"] = string_attributes[table_name]\n",
    "\n",
    "df_noise.createOrReplaceTempView(table_name+\"_noise\")\n",
    "\n",
    "#cols_included = [\"H\",\"X1B\",\"X2B\"]\n",
    "# numeric cols not having just zeros as values\n",
    "cols_included = [\"AB\", \"AVG\", \"BABIP\", \"BIP\", \"FB\", \"GB\", \"GIDP\", \"HBP\", \"HR\", \"H\", \"ISO\", \"LD\", \"OBP\",\n",
    "                 \"PA\", \"PU\", \"SF\", \"SH\", \"SLG\", \"SOL\", \"SOS\", \"SO\", \"TB\", \"X1B\", \"wOBA\", \"wRAA\", \"wRC\", \"year\"]\n",
    "\n",
    "for col_to_label in cols_included:\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    df = appr3_calc_similarities(table_name+\"_noise\", table_name, string_attributes, numeric_attributes, max_group_count, [col_to_label])\n",
    "    df_sorted = df.select(\"*\").sort(\"avg(EMD)\").collect()\n",
    "    true_labels.append(df_sorted[0][\"INNER_ATTR\"])\n",
    "    predicted_labels.append(df_sorted[0][\"OUTER_ATTR\"])\n",
    "\n",
    "\n",
    "    labeling_results={\n",
    "        \"labeled_table\":table_name+\"_noise\",\n",
    "        \"true_labels\": true_labels,\n",
    "        \"predicted_labels\":predicted_labels\n",
    "    }\n",
    "    with open(f\"results_emd/noise_compare/appro3/appro3_noise{data_perc_with_noise}_mgrc{max_group_count}_col_{col_to_label}.json\", \"w\") as outfile:\n",
    "        json.dump(labeling_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"results_emd/noise_compare/appro3/appro3_noise{data_perc_with_noise}_mgrc{max_group_count}_col_{col_to_label}.json\", \"w\") as outfile:\n",
    "        json.dump(labeling_results, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "table_name = \"MLB_1\"\n",
    "data_perc_with_noise = 0.1\n",
    "max_group_count = 3\n",
    "\n",
    "df_noise = add_additive_gaussian_noise(\n",
    "    table_name, data_perc_with_noise, random_seed, numeric_attributes)\n",
    "\n",
    "numeric_attributes[table_name+\"_noise\"] = numeric_attributes[table_name]\n",
    "string_attributes[table_name+\"_noise\"] = string_attributes[table_name]\n",
    "\n",
    "df_noise.createOrReplaceTempView(table_name+\"_noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = appr3_calc_similarities(table_name+\"_noise\", table_name, string_attributes, numeric_attributes, 3, [\"H\"])\n",
    "df.select(\"*\").sort(\"EMD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Matching and Labeling of cols based on similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlOuter = spark.sql(\"SELECT * FROM MLB_1\")\n",
    "wasserstein_distance(sqlOuter.select(collect_list(\"H\")).collect()[0][0], sqlOuter.select(collect_list(\"X1B\")).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to calc labeling performance with approach 1\n",
    "def appr1_calc_similarities(outer, inner, sel_attr):\n",
    "    join_attribute = list(set(string_attributes[inner]))\n",
    "    intersecting_attr = list(set(numeric_attributes[inner]))\n",
    "    sqlOuter = spark.sql(\"SELECT * FROM \" + outer)\n",
    "    # filter out null tupels with null values\n",
    "    sqlOuter = sqlOuter.dropna(subset=list(\n",
    "        map(lambda cur_col: \"`{cur_col}`\".format(cur_col=cur_col),\n",
    "            sqlOuter.columns)))\n",
    "    sqlInner = spark.sql(\"SELECT * FROM \"+inner)\n",
    "    # filter out null tupels with null values\n",
    "    sqlInner = sqlInner.dropna(subset=list(\n",
    "        map(lambda cur_col: \"`{cur_col}`\".format(cur_col=cur_col),\n",
    "            sqlInner.columns)))\n",
    "    attr_variations = pair_permutations_ordered(intersecting_attr)\n",
    "    # selsect specific attr_variation with a specific attribute included\n",
    "    #sel_attr = ['H','BB','X1B','X2B']\n",
    "    sel_attr = sel_attr\n",
    "    sel_attr_variations = list(\n",
    "         filter(lambda x: x[1] in sel_attr, attr_variations))\n",
    "\n",
    "    result = []         \n",
    "    for index_attr, curr_item in enumerate(sel_attr_variations):\n",
    "        print(str(index_attr) + \"/\" + str(len(sel_attr_variations)))\n",
    "        first_attr = curr_item[0]\n",
    "        second_attr = curr_item[1]\n",
    "        emd = wasserstein_distance(sqlOuter.select(collect_list(f\"`{first_attr}`\")).collect()[0][0],sqlInner.select(collect_list(f\"`{second_attr}`\")).collect()[0][0])\n",
    "        result.append([outer, first_attr, inner, second_attr, float(emd),0])\n",
    "    resultDF = spark.createDataFrame(result).toDF(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\",\"EMD\",\"count\")\n",
    "    return resultDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calc labeling performance with approach 3\n",
    "def appr3_calc_similarities(outer, inner, sel_attr):\n",
    "    outer = outer\n",
    "    inner = inner\n",
    "    join_attributes = list(set(string_attributes[inner]))\n",
    "    join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : \"o.`{join_att}` = i.`{join_att}`\".format(join_att=join_att) ,\\\n",
    "                                           join_attributes))\n",
    "    intersecting_attr = list(set(numeric_attributes[inner]))\n",
    "    #print(intersecting_attr)\n",
    "    #create projection list\n",
    "    projection_list = \" , \".join(\n",
    "        map(lambda attr: \"o.`{attr}` as `{attr}`\".format(attr=attr),\n",
    "            join_attributes)\n",
    "    ) + \" , \" + \" , \".join(\n",
    "        map(\n",
    "            lambda attr: \"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\".\n",
    "            format(attr=attr), intersecting_attr))\n",
    "    sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\n",
    "                            inner+ \" i \" + join_condition+\")\")\n",
    "    # filter out null tupels with null values\n",
    "    sqlDF = sqlDF.dropna(subset=list(\n",
    "        map(lambda cur_col: \"`{cur_col}`\".format(cur_col=cur_col),\n",
    "            sqlDF.columns)))\n",
    "    # calculates null values in the table\n",
    "    #sqlDF.select([count(when(isnan(f\"`{c}`\") | col(f\"`{c}`\").isNull(), c)).alias(f\"`{c}`\") for c in sqlDF.columns]).show()\n",
    "\n",
    "    # calcultes basic statisitc for the attributes\n",
    "    #print_df_to_html(sqlDF.describe())\n",
    "\n",
    "    attr_variations = pair_permutations_ordered(intersecting_attr)\n",
    "    #print(attr_variations)\n",
    "\n",
    "    # selsect specific attr_variation with a specific attribute included\n",
    "    #sel_attr = ['H','BB','X1B','X2B']\n",
    "    sel_attr = sel_attr\n",
    "    sel_attr_variations = list(\n",
    "         filter(lambda x: x[1] in sel_attr, attr_variations))\n",
    "    #print(sel_attr_variations)\n",
    "    \n",
    "    #sel_attr_variations = [['H', \"H\"]]\n",
    "\n",
    "    for index_attr, curr_item in enumerate(sel_attr_variations):\n",
    "        print(str(index_attr) + \"/\" + str(len(sel_attr_variations)))\n",
    "        first_attr = curr_item[0]\n",
    "        second_attr = curr_item[1]\n",
    "        #print(first_attr)\n",
    "        #print(second_attr)\n",
    "        if index_attr == 0:\n",
    "            curDF = sqlDF.groupby(join_attributes).agg(\n",
    "                emd_UDF(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr)),\n",
    "                        collect_list(\"`i.{second_attr}`\".format(second_attr=second_attr))).alias(\"EMD\"),\n",
    "                count(\"`i.{second_attr}`\".format(second_attr=second_attr)).alias(\"count\")).select(col(\"EMD\"), col(\"count\"))\n",
    "            curDF = curDF.withColumn(\"OUTER\", lit(outer)).withColumn(\n",
    "                \"OUTER_ATTR\",\n",
    "                lit(first_attr)).withColumn(\"INNER\", lit(inner)).withColumn(\n",
    "                    \"INNER_ATTR\", lit(second_attr))\n",
    "        else:\n",
    "            newDF = sqlDF.groupby(join_attributes).agg(\n",
    "                emd_UDF(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr)),\n",
    "                        collect_list(\"`i.{second_attr}`\".format(second_attr=second_attr))).alias(\"EMD\"),\n",
    "                count(\"`i.{second_attr}`\".format(second_attr=second_attr)).alias(\"count\")).select(col(\"EMD\"), col(\"count\"))\n",
    "            newDF = newDF.withColumn(\"OUTER\", lit(outer)).withColumn(\n",
    "                \"OUTER_ATTR\",\n",
    "                lit(first_attr)).withColumn(\"INNER\", lit(inner)).withColumn(\n",
    "                    \"INNER_ATTR\", lit(second_attr))\n",
    "            curDF = curDF.union(newDF)\n",
    "    return curDF\n",
    "    # curDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\n",
    "    #     \"/HdiNotebooks/semantic_data_lake/results/emd_results_dist_sep_inst_{outer}_{inner}\".format(\n",
    "    #         outer=outer, inner=inner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calc labeling performance with approach 4\n",
    "def appr4_calc_similarities(outer, inner, sel_attr, max_group_counts):\n",
    "    outer = outer\n",
    "    inner = inner\n",
    "    # find matching attributes to compare\n",
    "    join_attributes = list(set(string_attributes[inner]))\n",
    "    join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : \"o.`{join_att}` = i.`{join_att}`\".format(join_att=join_att) ,\\\n",
    "                                           join_attributes))\n",
    "    intersecting_attr = list(set(numeric_attributes[inner]))\n",
    "    #print(intersecting_attr)\n",
    "    #create projection list\n",
    "    projection_list = \" , \".join(\n",
    "        map(lambda attr: \"o.`{attr}` as `{attr}`\".format(attr=attr),\n",
    "            join_attributes)\n",
    "    ) + \" , \" + \" , \".join(\n",
    "        map(\n",
    "            lambda attr: \"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\".\n",
    "            format(attr=attr), intersecting_attr))\n",
    "    sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\n",
    "                            inner+ \" i \" + join_condition+\")\")\n",
    "    # filter out null tupels with null values\n",
    "    sqlDF = sqlDF.dropna(subset=list(\n",
    "        map(lambda cur_col: \"`{cur_col}`\".format(cur_col=cur_col),\n",
    "            sqlDF.columns)))\n",
    "\n",
    "    result_list=[]\n",
    "    #for max_group_count in list(range(1,9,2))+list(range(10,150,20)):\n",
    "    for max_group_count in max_group_counts:\n",
    "        print(max_group_count)\n",
    "        \n",
    "        # groupby string attribute and filter out instances which only consider specific times\n",
    "        sqlDF_instances_to_consider = sqlDF.groupby(join_attributes).agg(count(join_attributes[0]).alias(\"count\")).where(col(\"count\") <= max_group_count)\n",
    "        resSqlDF = sqlDF.join(sqlDF_instances_to_consider, on=join_attributes, how='inner')\n",
    "\n",
    "        attr_variations = pair_permutations_ordered(intersecting_attr)\n",
    "        #print(attr_variations)\n",
    "\n",
    "        # selsect specific attr_variation with a specific attribute included\n",
    "        sel_attr = sel_attr\n",
    "        sel_attr_variations = list(\n",
    "            filter(lambda x: x[1] in sel_attr, attr_variations))\n",
    "        #print(sel_attr_variations)\n",
    "        #print(sel_attr_variations)\n",
    "        #print(len(sel_attr_variations))\n",
    "\n",
    "        #sel_attr_variations = [['H', \"H\"]]\n",
    "        \n",
    "        for index_attr, curr_item in enumerate(sel_attr_variations):\n",
    "            print(str(index_attr)+\"/\"+str(len(sel_attr_variations)))\n",
    "            first_attr = curr_item[0]\n",
    "            second_attr = curr_item[1]\n",
    "            # print(first_attr)\n",
    "            # print(second_attr)\n",
    "            emd = resSqlDF.select(emd_UDF(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr)),collect_list(\"`i.{second_attr}`\".format(second_attr=second_attr))).alias(\"EMD\")).collect()[0][\"EMD\"]\n",
    "            #print([outer,first_attr, inner, second_attr, max_group_count, float(emd)])\n",
    "            result_list.append([outer,first_attr, inner, second_attr, max_group_count, float(emd)])\n",
    "        resultDF = spark.createDataFrame(result_list).toDF(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\",\"COUNT\",\"EMD\")\n",
    "        #resultDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"/HdiNotebooks/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}/emd_results_dist_sep_inst_appr4_maxgr{max_group_count}_{first_attr}_{second_attr}\".format(outer=outer, inner=inner, max_group_count=max_group_count, first_attr=first_attr, second_attr=second_attr))\n",
    "    return resultDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDF = appr4_calc_similarities()\n",
    "#classification_report_labeling(curDF, [\"H\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = curDF.select(\"*\").where(col(\"INNER_ATTR\") == \"H\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\")\n",
    "print(df.collect()[0][\"OUTER_ATTR\"])\n",
    "print(df.collect()[0][\"avg(EMD)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDF.select(\"*\").where(col(\"INNER_ATTR\") == \"H\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Noise: exchange colum values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_table = \"MLB_1\"\n",
    "# first try to add noise to one column by selecting values from other column and replace the values\n",
    "df = spark.sql(\"SELECT * from {ac_table}\".format(ac_table=ac_table))\n",
    "df_noise = df\n",
    "\n",
    "# want bring 10% noise in it\n",
    "noise_frac = 0.15\n",
    "# calc how many values are needed to bring x% noise in one column\n",
    "#number_of_noisevalues = int(df.count()*noise_frac)\n",
    "\n",
    "\n",
    "# changing values for H for test\n",
    "dic_noise = {}\n",
    "# adding the string attributes initialy\n",
    "for string_attribute in string_attributes[\"MLB_1\"]:\n",
    "    dic_noise[string_attribute] = []\n",
    "for index,numeric_attribute_1 in enumerate(numeric_attributes[\"MLB_1\"]):\n",
    "    print(numeric_attribute_1)\n",
    "    dic_noise[numeric_attribute_1] = []\n",
    "    if index > 0:\n",
    "        for string_attribute in string_attributes[\"MLB_1\"]:\n",
    "            dic_noise[string_attribute].extend(df.select(col(\"`\"+string_attribute+\"`\")).sample(noise_frac/(len(numeric_attributes[\"MLB_1\"])-1),0).rdd.flatMap(list).collect())\n",
    "    for numeric_attribute_2 in numeric_attributes[\"MLB_1\"]:\n",
    "        #print(numeric_attribute_1)\n",
    "        if numeric_attribute_2 == numeric_attribute_1:\n",
    "            continue\n",
    "        dic_noise[numeric_attribute_1].extend(df.select(col(\"`\"+numeric_attribute_2+\"`\")).sample(noise_frac/(len(numeric_attributes[\"MLB_1\"])-1),0).rdd.flatMap(list).collect())\n",
    "\n",
    "\n",
    "pd.DataFrame(dic_noise).to_csv(file_path+\"MLB_1_noise\"+str(noise_frac*100)+\".csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dic_noise[\"batter_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "\n",
    "split = df.randomSplit([0.8,0.2],0)\n",
    "print(split[0].count()/df.count())\n",
    "print(split[1].count()/df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding noise similiar to Valentine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"MLB_1\"\n",
    "random_plus_minus_range = 10\n",
    "\n",
    "df = spark.sql(\"SELECT * from MLB_1\")\n",
    "# adding noise\n",
    "for numeric_attribute in numeric_attributes[table_name]:\n",
    "    #print(numeric_attribute)\n",
    "    #if numeric_attribute != \"AB\":\n",
    "    #    break\n",
    "    \n",
    "    df_stats = spark.sql(\"SELECT mean(`{numeric_attribute}`) as mean, stddev(`{numeric_attribute}`) as stddev from {table_name}\".format(\n",
    "        numeric_attribute=numeric_attribute, table_name=table_name)).collect()[0]\n",
    "    df_col_mean = df_stats[\"mean\"]\n",
    "    df_col_stddev = df_stats[\"stddev\"]\n",
    "    #print(df_col_mean, df_col_stddev)\n",
    "    df_col_mean_new = float(df_col_mean) + float(df_col_mean) * random.uniform(-random_plus_minus_range, random_plus_minus_range)/100\n",
    "    df_col_stddev_new = float(df_col_stddev) + float(df_col_stddev) * random.uniform(-random_plus_minus_range, random_plus_minus_range)/100\n",
    "    #print(df_col_mean_new, df_col_stddev_new)\n",
    "    new_col = np.random.normal(df_col_mean_new, df_col_stddev_new, df.count())\n",
    "    #print(df.count())\n",
    "    #print(len(new_col))\n",
    "    #print(new_col.mean())\n",
    "    df_col_new = spark.createDataFrame([(float(l),) for l in new_col], schema=StructType([\n",
    "                                       StructField(\"new_col\", FloatType())]))\n",
    "    df_col_new = df_col_new.withColumn('row_index', row_number().over(\n",
    "        Window.orderBy(monotonically_increasing_id())))\n",
    "    df = df.withColumn('row_index', row_number().over(\n",
    "        Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "    df = df.join(df_col_new, on=[\"row_index\"]).drop(\"row_index\")\n",
    "    df = df.withColumn(\"{numeric_attribute}\".format(numeric_attribute=numeric_attribute), df.new_col).drop(\"new_col\")\n",
    "df.createOrReplaceTempView(table_name+\"_noise\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT mean(H) from MLB_1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT mean(H) from MLB_1_noise\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additive Gaussian Noise with N(0,stdd)\n",
    "- stdd => standard deviation of the specific column, where noise should be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seed = 0\n",
    "# np.random.seed(random_seed)\n",
    "# table_name = \"MLB_1\"\n",
    "# data_perc_with_noise = 0.1\n",
    "\n",
    "# df_original = spark.sql(f\"SELECT * from {table_name}\")\n",
    "\n",
    "def add_additive_gaussian_noise(table_name, data_perc_with_noise, random_seed):\n",
    "  df_original = spark.sql(f\"SELECT * from {table_name}\")\n",
    "  df_without_noise, df_to_add_noise = df_original.randomSplit([1-data_perc_with_noise, data_perc_with_noise],random_seed)\n",
    "  #print(df.count(),df_without_noise.count(), df_to_add_noise.count())\n",
    "  df_to_add_noise_pd = df_to_add_noise.toPandas()\n",
    "\n",
    "  # adding noise to every numeric column\n",
    "  for numeric_attribute in numeric_attributes[table_name]:\n",
    "      #print(numeric_attribute)\n",
    "      #if numeric_attribute != \"AB\":\n",
    "        #  break\n",
    "      df_stats = df_original.select(mean(f\"`{numeric_attribute}`\").alias(\"mean\"), stddev(f\"`{numeric_attribute}`\").alias(\"stddev\")).collect()[0]\n",
    "      df_col_mean = df_stats[\"mean\"]\n",
    "      df_col_stddev = df_stats[\"stddev\"]\n",
    "\n",
    "      for index, row in df_to_add_noise_pd.iterrows():\n",
    "          if row[f\"{numeric_attribute}\"] != None: # filter None values\n",
    "            df_to_add_noise_pd.at[index, f\"{numeric_attribute}\"] = float(row[f\"{numeric_attribute}\"]) + float(np.random.normal(0,df_col_stddev,1)[0])\n",
    "\n",
    "  df_to_add = spark.createDataFrame(df_to_add_noise_pd)\n",
    "  df_noise_full = df_without_noise.union(spark.createDataFrame(df_to_add_noise_pd))\n",
    "  return df_noise_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Matching and Labeling with noisy data (additive noise with N(0,stdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPROACH 1\n",
    "# duration of about 7 hours\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "table_name = \"MLB_1\"\n",
    "data_perc_with_noise = 0.5\n",
    "max_group_count = 0\n",
    "\n",
    "\n",
    "for data_perc_with_noise in [0.01]:\n",
    "    df_noise = add_additive_gaussian_noise(table_name, data_perc_with_noise, random_seed)\n",
    "\n",
    "    df_noise.createOrReplaceTempView(table_name+\"_noise\")\n",
    "\n",
    "    #cols_included = [\"H\",\"X1B\",\"X2B\"]\n",
    "    # numeric cols not having just zeros as values\n",
    "    cols_included = [\"AB\", \"AVG\", \"BABIP\", \"BIP\", \"FB\", \"GB\", \"GIDP\", \"HBP\", \"HR\", \"H\", \"ISO\", \"LD\", \"OBP\",\n",
    "            \"PA\", \"PU\", \"SF\", \"SH\", \"SLG\", \"SOL\", \"SOS\", \"SO\", \"TB\", \"X1B\", \"wOBA\", \"wRAA\", \"wRC\", \"year\"]\n",
    "\n",
    "    #cols_included = [\"FB\", \"GB\", \"HR\", \"H\", \"LD\", \"X1B\",\"X2B\", \"X3B\",\"wRC\", \"year\"]\n",
    "    #cols_included = [\"H\",\"X1B\",\"X2B\",\"X3B\",\"wRC\"]\n",
    "\n",
    "    df = appr1_calc_similarities(table_name+\"_noise\", table_name, cols_included)\n",
    "\n",
    "    class_report = classification_report_labeling(df, cols_included, max_group_count)\n",
    "\n",
    "    pickle.dump(class_report, open(f\"results_emd/noise_compare/appro1_noise{data_perc_with_noise}_mgrc{max_group_count}_all_columns.p\", \"wb\"))\n",
    "    #pickle.dump(class_report, open(f\"semantic_data_lake/semantic_data_lake/results/noise_compare/appro3_noise{data_perc_with_noise}_mgrc{max_group_count}_all_columns.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPROACH 3 \n",
    "# duration of about 7 hours\n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "table_name = \"MLB_1\"\n",
    "data_perc_with_noise = 0.5\n",
    "max_group_count = 3\n",
    "\n",
    "df_noise = add_additive_gaussian_noise(table_name, data_perc_with_noise, random_seed)\n",
    "\n",
    "df_noise.createOrReplaceTempView(table_name+\"_noise\")\n",
    "\n",
    "#cols_included = [\"H\",\"X1B\",\"X2B\"]\n",
    "# numeric cols not having just zeros as values\n",
    "cols_included = [\"AB\", \"AVG\", \"BABIP\", \"BIP\", \"FB\", \"GB\", \"GIDP\", \"HBP\", \"HR\", \"H\", \"ISO\", \"LD\", \"OBP\",\n",
    "        \"PA\", \"PU\", \"SF\", \"SH\", \"SLG\", \"SOL\", \"SOS\", \"SO\", \"TB\", \"X1B\", \"wOBA\", \"wRAA\", \"wRC\", \"year\"]\n",
    "\n",
    "#cols_included = [\"FB\", \"GB\", \"HR\", \"H\", \"LD\", \"X1B\",\"X2B\", \"X3B\",\"wRC\", \"year\"]\n",
    "#cols_included = [\"H\",\"X1B\",\"X2B\",\"X3B\",\"wRC\"]\n",
    "\n",
    "df = appr3_calc_similarities(table_name+\"_noise\", table_name, cols_included)\n",
    "\n",
    "class_report = classification_report_labeling(df, cols_included, max_group_count)\n",
    "\n",
    "pickle.dump(class_report, open(f\"results_emd/noise_compare/appro3_noise{data_perc_with_noise}_mgrc{max_group_count}_all_columns.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 4\n",
    "# duration of about \n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "table_name = \"MLB_1\"\n",
    "data_perc_with_noise = 0.2\n",
    "max_group_count = [3]\n",
    "\n",
    "for data_perc_with_noise in [0.62,0.65,0.67,0.75,0.95]:\n",
    "    \n",
    "    df_noise = add_additive_gaussian_noise(table_name, data_perc_with_noise, random_seed)\n",
    "\n",
    "    df_noise.createOrReplaceTempView(table_name+\"_noise\")\n",
    "\n",
    "    #cols_included = [\"H\",\"X1B\",\"X2B\"]\n",
    "    # numeric cols not having just zeros as values\n",
    "    #cols_included = [\"AB\", \"AVG\", \"BABIP\", \"BIP\", \"FB\", \"GB\", \"GIDP\", \"HBP\", \"HR\", \"H\", \"ISO\", \"LD\", \"OBP\",\n",
    "    #        \"PA\", \"PU\", \"SF\", \"SH\", \"SLG\", \"SOL\", \"SOS\", \"SO\", \"TB\", \"X1B\", \"wOBA\", \"wRAA\", \"wRC\", \"year\"]\n",
    "\n",
    "    #cols_included = [\"FB\", \"GB\", \"HR\", \"H\", \"LD\", \"X1B\",\"X2B\", \"X3B\",\"wRC\", \"year\"]\n",
    "    cols_included = [\"H\",\"X1B\",\"X2B\",\"X3B\",\"wRC\"]\n",
    "\n",
    "    df = appr4_calc_similarities(table_name+\"_noise\", table_name, cols_included, max_group_count)\n",
    "\n",
    "    class_report = classification_report_labeling(df, cols_included, max_group_count[0])\n",
    "\n",
    "    pickle.dump(class_report, open(f\"results_emd/noise_compare/appro4_noise{data_perc_with_noise}_mgrc{max_group_count[0]}.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Approach 4\n",
    "# duration of about \n",
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "table_name = \"MLB_1\"\n",
    "data_perc_with_noise = 0.2\n",
    "max_group_count = [3]\n",
    "\n",
    "for data_perc_with_noise in [0.01,0.1,0.2,0.3,0.4,0.6,0.7,0.8,0.9,0.99]:\n",
    "#for data_perc_with_noise in [0.1]:\n",
    "    df_noise = add_additive_gaussian_noise(table_name, data_perc_with_noise, random_seed)\n",
    "\n",
    "    df_noise.createOrReplaceTempView(table_name+\"_noise\")\n",
    "\n",
    "    #cols_included = [\"H\",\"X1B\",\"X2B\"]\n",
    "    # numeric cols not having just zeros as values\n",
    "    cols_included = [\"AB\", \"AVG\", \"BABIP\", \"BIP\", \"FB\", \"GB\", \"GIDP\", \"HBP\", \"HR\", \"H\", \"ISO\", \"LD\", \"OBP\",\n",
    "            \"PA\", \"PU\", \"SF\", \"SH\", \"SLG\", \"SOL\", \"SOS\", \"SO\", \"TB\", \"X1B\", \"wOBA\", \"wRAA\", \"wRC\", \"year\"]\n",
    "\n",
    "    #cols_included = [\"FB\", \"GB\", \"HR\", \"H\", \"LD\", \"X1B\",\"X2B\", \"X3B\",\"wRC\", \"year\"]\n",
    "    #cols_included = [\"H\",\"X1B\",\"X2B\",\"X3B\",\"wRC\"]\n",
    "\n",
    "    df = appr4_calc_similarities(table_name+\"_noise\", table_name, cols_included, max_group_count)\n",
    "\n",
    "    class_report = classification_report_labeling(df, cols_included, max_group_count[0])\n",
    "\n",
    "    pickle.dump(class_report, open(f\"results_emd/noise_compare/appro4_noise{data_perc_with_noise}_mgrc{max_group_count[0]}_all_columns.p\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing Matching and Labeling with noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "noise_perc = 0.01 # 0.01 => 1% noise adding\n",
    "table_name = \"MLB_1\"\n",
    "# Build the datafram with noise\n",
    "# first cut the noise_percentage from the original data\n",
    "projection_list = \" , \".join(\n",
    "        map(lambda attr: \"`{attr}`\".format(attr=attr),\n",
    "            numeric_attributes[table_name])\n",
    "    )\n",
    "df_noise_comp = spark.sql(\"SELECT \"+projection_list +\" FROM {table}\".format(table=table)).randomSplit([(1-noise_perc),noise_perc],random_seed)[0]\n",
    "#df_noise = df_noise.select(numeric_attributes[table_name])\n",
    "# read the random selected noise\n",
    "data_file = file_path + table_name + \"_noise\"+str(noise_perc*100) + \".csv\"\n",
    "\n",
    "header_file = file_path + \"samples/\" + table_name + \".header.csv\"\n",
    "datatype_file = file_path + \"samples/\" + table_name + \".datatypes.csv\"\n",
    "# create a DataFrame using an ifered Schema\n",
    "df_noise_to_add = spark.read.option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".option(\"delimiter\", \",\") \\\n",
    ".csv(data_file).toDF(*numeric_attributes[table_name])\n",
    "\n",
    "print(df_noise.count())\n",
    "print(df_noise_to_add.count())\n",
    "df_noise = df_noise.union(df_noise_to_add)\n",
    "print(df_noise.count())\n",
    "\n",
    "df_noise.createOrReplaceTempView(table_name+\"_noise\")\n",
    "\n",
    "outer = table_name\n",
    "inner = table_name+\"_noise\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.sql(\"SELECT * from MLB_1\").count())\n",
    "print(spark.sql(\"SELECT * from MLB_1_noise\").count())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57d51e845e90cb7046bb59f8f3e568dcb7335eb60d8163d91d0093f69686e60"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
