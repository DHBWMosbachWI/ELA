{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "sys.path.insert(0, \"D:\\\\semantic_data_lake\\\\semantic_data_lake\")\r\n",
    "from pyspark import SparkConf, SparkContext\r\n",
    "from pyspark.sql import SparkSession, DataFrame\r\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType\r\n",
    "from pyspark.sql.functions import udf, col, pandas_udf, PandasUDFType, collect_list, count, avg, lit\r\n",
    "from scipy.stats import wasserstein_distance\r\n",
    "from numpy import asarray\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pickle\r\n",
    "\r\n",
    "from itertools import permutations, combinations, combinations_with_replacement\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from helper_functions import print_df_to_html, translate_header_file_to_list, variations, pair_permutations_ordered, translate_datatype_file_to_list, cast_datatypes, check_attribute_completeness, compare_schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Config\r\n",
    "conf = SparkConf()\r\n",
    "conf.set(\"spark.executor.instances\",\"4\")\r\n",
    "conf.set(\"spark.executor.cores\",\"4\")\r\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\r\n",
    "conf.set(\"spark.driver.memory\", \"8g\")\r\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\r\n",
    "conf.set(\"spark.memory.offHeap.size\", \"16g\")\r\n",
    "conf.setMaster(\"local[4]\")\r\n",
    "conf.setAppName(\"MLB-similarity-calc\")\r\n",
    "# create a SparkSession\r\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = [1,2,3,4,5]\r\n",
    "\r\n",
    "\r\n",
    "def printer(x):\r\n",
    "    #pd.DataFrame([x]).to_csv(\"D:\\\\test_\"+x+\".csv\")\r\n",
    "    print(x)\r\n",
    " \r\n",
    "rdd = spark.sparkContext.parallelize(data_array)\r\n",
    "\r\n",
    "test = rdd.foreach(printer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Register UDF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=FloatType())\r\n",
    "def emd_UDF(col1, col2) -> FloatType:\r\n",
    "    return float(wasserstein_distance(col1, col2))\r\n",
    "\r\n",
    "spark.udf.register(\"emd_UDF\", emd_UDF)\r\n",
    "\r\n",
    "# sqlDF_MLB_1 = spark.sql(\"SELECT * from MLB_1\")\r\n",
    "# sqlDF_MLB_1.groupby([\"batter_name\", \"teamname\", \"parentteam\", \"league\"]).agg(\r\n",
    "#     emd_UDF(collect_list(\"H\"), collect_list(\"H\")).alias(\"EMD\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_REL_PATH = \"semantic_data_lake/semantic_data_lake/data/benchmark/\"\r\n",
    "#BENCHMARK_REL_PATH = \"data/benchmark/\"\r\n",
    "\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_20\", \"MLB_15\"]\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_20\"]\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_10\"]\r\n",
    "list_of_all_MLB_tables = [\r\n",
    "    \"MLB_1\", \"MLB_10\", \"MLB_11\", \"MLB_12\", \"MLB_13\", \"MLB_14\", \"MLB_15\",\r\n",
    "    \"MLB_16\", \"MLB_17\", \"MLB_18\", \"MLB_19\", \"MLB_2\", \"MLB_20\", \"MLB_21\",\r\n",
    "    \"MLB_22\", \"MLB_23\", \"MLB_24\", \"MLB_25\", \"MLB_26\", \"MLB_27\", \"MLB_28\",\r\n",
    "    \"MLB_29\", \"MLB_3\", \"MLB_30\", \"MLB_31\", \"MLB_32\", \"MLB_33\", \"MLB_34\",\r\n",
    "    \"MLB_35\", \"MLB_36\", \"MLB_37\", \"MLB_38\", \"MLB_39\", \"MLB_4\", \"MLB_40\",\r\n",
    "    \"MLB_41\", \"MLB_42\", \"MLB_43\", \"MLB_44\", \"MLB_45\", \"MLB_46\", \"MLB_47\",\r\n",
    "    \"MLB_48\", \"MLB_49\", \"MLB_5\", \"MLB_50\", \"MLB_51\", \"MLB_52\", \"MLB_53\",\r\n",
    "    \"MLB_54\", \"MLB_55\", \"MLB_56\", \"MLB_57\", \"MLB_58\", \"MLB_59\", \"MLB_6\",\r\n",
    "    \"MLB_60\", \"MLB_61\", \"MLB_62\", \"MLB_63\", \"MLB_64\", \"MLB_65\", \"MLB_66\",\r\n",
    "    \"MLB_67\", \"MLB_68\", \"MLB_7\", \"MLB_8\", \"MLB_9\"\r\n",
    "]\r\n",
    "#list_of_MLB_join_candidate_pairs = [ (\"MLB_1\",\"MLB_12\"), (\"MLB_1\",\"MLB_13\"), (\"MLB_1\",\"MLB_14\")  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(combinations(list_of_all_MLB_tables,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = BENCHMARK_REL_PATH + \"MLB/\"\n",
    "sample = False\n",
    "\n",
    "# dict of string attributes for each table\n",
    "string_attributes = {}\n",
    "numeric_attributes = {}\n",
    "for table_name in list_of_all_MLB_tables:\n",
    "    if sample:\n",
    "        data_file = file_path + \"samples/\" + table_name + \".sample\" + \".csv\"\n",
    "    else:\n",
    "        data_file = file_path + table_name + \".csv\"\n",
    "    header_file = file_path + \"samples/\" + table_name + \".header.csv\"\n",
    "    datatype_file = file_path + \"samples/\" + table_name + \".datatypes.csv\"\n",
    "    # create a DataFrame using an ifered Schema\n",
    "    orig_df = spark.read.option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .csv(data_file).toDF(*translate_header_file_to_list(header_file))\n",
    "    df = cast_datatypes(datatype_file, orig_df)\n",
    "    # compare_schemas(orig_df, df)\n",
    "    df.createOrReplaceTempView(table_name)\n",
    "    string_attributes[table_name] = list(filter(lambda x : not x.startswith(\"Calculation\"), \\\n",
    "                                    map(lambda x : x[0], filter(lambda tupel: tupel[1] == 'string' ,df.dtypes))))\n",
    "    numeric_attributes[table_name] = list(filter(lambda x : not x.startswith(\"Calculation\"), \\\n",
    "                                       map(lambda x : x[0], \\\n",
    "                                           filter(lambda tupel: tupel[1] == 'double' or \\\n",
    "                                           tupel[1] == 'int' or tupel[1].startswith('decimal'),df.dtypes))))\n",
    "    check_attribute_completeness(df.columns, string_attributes[table_name],\n",
    "                                 numeric_attributes[table_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = \"MLB_1\"\r\n",
    "inner = \"MLB_10\"\r\n",
    "# find matching attributes to compare\r\n",
    "join_attributes = list(\r\n",
    "    set(string_attributes[outer]) & set(string_attributes[inner]))\r\n",
    "print(join_attributes)\r\n",
    "join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : f\"o.`{join_att}` = i.`{join_att}`\" ,\\\r\n",
    "                                           join_attributes))\r\n",
    "print(join_condition)\r\n",
    "intersecting_attr = list(\r\n",
    "    set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "intersecting_attr = list([\"H\"])\r\n",
    "print(intersecting_attr)\r\n",
    "#create projection list\r\n",
    "projection_list = \" , \".join(\r\n",
    "    map(lambda attr: f\"o.`{attr}` as `{attr}`\", join_attributes)\r\n",
    ") + \" , \" + \" , \".join(\r\n",
    "    map(lambda attr: f\"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\",\r\n",
    "        intersecting_attr))\r\n",
    "print(projection_list)\r\n",
    "sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\r\n",
    "                        inner+ \" i \" + join_condition+\")\")\r\n",
    "sqlDF = sqlDF.dropna(\r\n",
    "    subset=list(map(lambda cur_col: f\"`{cur_col}`\", sqlDF.columns)))\r\n",
    "\r\n",
    "sqlDF_instances_to_consider = sqlDF.groupby(join_attributes).agg(count(join_attributes[0]).alias(\"count\")).where(col(\"count\") <= 1)\r\n",
    "print(sqlDF_instances_to_consider.count())\r\n",
    "#resSqlDF = sqlDF.join(sqlDF_instances_to_consider, on=join_attributes, how='inner')\r\n",
    "#emd = resSqlDF.select(emd_UDF(collect_list('`o.H`'),collect_list('`i.H`')).alias(\"EMD\")).collect()[0][\"EMD\"]\r\n",
    "#rint(emd)\r\n",
    "#attr_outer = resSqlDF.select(collect_list('`o.H`')).collect()[0][0]\r\n",
    "#attr_inner = resSqlDF.select(collect_list('`i.H`')).collect()[0][0]\r\n",
    "#print(wasserstein_distance(attr_outer,attr_inner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### AZURE VERSION #############################\r\n",
    "## Calc EMD and COS just for selected combinations\r\n",
    "for index, curr_set in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "    # if index > 1:\r\n",
    "    #     break\r\n",
    "    outer = curr_set[0]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        break\r\n",
    "    inner = curr_set[1]\r\n",
    "    # skip combinations that are already calculated\r\n",
    "    if inner in [\"MLB_1\", \"MLB_10\", \"MLB_11\", \"MLB_12\", \"MLB_13\", \"MLB_14\", \"MLB_15\",\r\n",
    "    \"MLB_16\", \"MLB_17\", \"MLB_18\", \"MLB_19\", \"MLB_2\", \"MLB_20\", \"MLB_21\",\r\n",
    "    \"MLB_22\", \"MLB_23\", \"MLB_24\", \"MLB_25\", \"MLB_26\", \"MLB_27\", \"MLB_28\",\r\n",
    "    \"MLB_29\", \"MLB_3\", \"MLB_30\", \"MLB_31\", \"MLB_32\", \"MLB_33\", \"MLB_34\",\r\n",
    "    \"MLB_35\", \"MLB_36\", \"MLB_37\", \"MLB_38\", \"MLB_39\", \"MLB_4\", \"MLB_40\",\r\n",
    "    \"MLB_41\", \"MLB_42\", \"MLB_43\", \"MLB_44\", \"MLB_6\",\r\n",
    "    \"MLB_60\", \"MLB_61\", \"MLB_62\", \"MLB_63\", \"MLB_64\", \"MLB_65\", \"MLB_66\",\r\n",
    "    \"MLB_67\", \"MLB_68\", \"MLB_7\", \"MLB_8\", \"MLB_9\"]:\r\n",
    "        continue\r\n",
    "    print(outer,inner)\r\n",
    "    # find matching attributes to compare\r\n",
    "    join_attributes = list(\r\n",
    "        set(string_attributes[inner]) & set(string_attributes[outer]))\r\n",
    "    join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : \"o.`{join_att}` = i.`{join_att}`\".format(join_att=join_att) ,\\\r\n",
    "                                           join_attributes))\r\n",
    "    intersecting_attr = list(\r\n",
    "        set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "    #print(intersecting_attr)\r\n",
    "    #create projection list\r\n",
    "    projection_list = \" , \".join(\r\n",
    "        map(lambda attr: \"o.`{attr}` as `{attr}`\".format(attr=attr),\r\n",
    "            join_attributes)\r\n",
    "    ) + \" , \" + \" , \".join(\r\n",
    "        map(\r\n",
    "            lambda attr: \"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\".\r\n",
    "            format(attr=attr), intersecting_attr))\r\n",
    "    sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\r\n",
    "                            inner+ \" i \" + join_condition+\")\")\r\n",
    "    # filter out null tupels with null values\r\n",
    "    sqlDF = sqlDF.dropna(subset=list(\r\n",
    "        map(lambda cur_col: \"`{cur_col}`\".format(cur_col=cur_col),\r\n",
    "            sqlDF.columns)))\r\n",
    "\r\n",
    "    #for max_group_count in list(range(1,9,2))+list(range(10,150,20)):\r\n",
    "    for max_group_count in [3]:\r\n",
    "        print(max_group_count)\r\n",
    "        \r\n",
    "        # groupby string attribute and filter out instances which only consider specific times\r\n",
    "        sqlDF_instances_to_consider = sqlDF.groupby(join_attributes).agg(count(join_attributes[0]).alias(\"count\")).where(col(\"count\") <= max_group_count)\r\n",
    "        resSqlDF = sqlDF.join(sqlDF_instances_to_consider, on=join_attributes, how='inner')\r\n",
    "\r\n",
    "        attr_variations = pair_permutations_ordered(intersecting_attr)\r\n",
    "        #print(attr_variations)\r\n",
    "\r\n",
    "        # selsect specific attr_variation with a specific attribute included\r\n",
    "        sel_attr = ['H','BB','X1B','X2B']\r\n",
    "        sel_attr_variations = list(\r\n",
    "            filter(lambda x: x[1] in sel_attr, attr_variations))\r\n",
    "        #print(sel_attr_variations)\r\n",
    "        #print(sel_attr_variations)\r\n",
    "        #print(len(sel_attr_variations))\r\n",
    "\r\n",
    "        #sel_attr_variations = [['H', \"H\"]]\r\n",
    "        \r\n",
    "        for index_attr, curr_item in enumerate(sel_attr_variations):\r\n",
    "            result_list=[]\r\n",
    "            print(str(index_attr)+\"/\"+str(len(sel_attr_variations)))\r\n",
    "            first_attr = curr_item[0]\r\n",
    "            second_attr = curr_item[1]\r\n",
    "            print(first_attr, second_attr)\r\n",
    "            emd = resSqlDF.select(emd_UDF(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr)),collect_list(\"`i.{second_attr}`\".format(second_attr=second_attr))).alias(\"EMD\")).collect()[0][\"EMD\"]\r\n",
    "            #print([outer,first_attr, inner, second_attr, max_group_count, float(emd)])\r\n",
    "            result_list.append([outer,first_attr, inner, second_attr, max_group_count, float(emd)])\r\n",
    "            resultDF = spark.createDataFrame(result_list).toDF(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\",\"COUNT\",\"EMD\")\r\n",
    "            resultDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}/emd_results_dist_sep_inst_appr4_maxgr{max_group_count}_{first_attr}_{second_attr}\".format(outer=outer, inner=inner, max_group_count=max_group_count, first_attr=first_attr, second_attr=second_attr))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### AZURE VERSION #############################\r\n",
    "## Calc EMD for selected combinations with our defined approach 4\r\n",
    "for index, curr_set in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "    # if index > 1:\r\n",
    "    #     break\r\n",
    "    outer = curr_set[0]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        break\r\n",
    "    inner = curr_set[1]\r\n",
    "    # skip combinations that are already calculated\r\n",
    "    if inner in []:\r\n",
    "        continue\r\n",
    "    print(outer,inner)\r\n",
    "    # find matching attributes to compare\r\n",
    "    join_attributes = list(\r\n",
    "        set(string_attributes[inner]) & set(string_attributes[outer]))\r\n",
    "    join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : \"o.`{join_att}` = i.`{join_att}`\".format(join_att=join_att) ,\\\r\n",
    "                                           join_attributes))\r\n",
    "    intersecting_attr = list(\r\n",
    "        set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "    #print(intersecting_attr)\r\n",
    "    #create projection list\r\n",
    "    projection_list = \" , \".join(\r\n",
    "        map(lambda attr: \"o.`{attr}` as `{attr}`\".format(attr=attr),\r\n",
    "            join_attributes)\r\n",
    "    ) + \" , \" + \" , \".join(\r\n",
    "        map(\r\n",
    "            lambda attr: \"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\".\r\n",
    "            format(attr=attr), intersecting_attr))\r\n",
    "    sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\r\n",
    "                            inner+ \" i \" + join_condition+\")\")\r\n",
    "    # filter out null tupels with null values\r\n",
    "    sqlDF = sqlDF.dropna(subset=list(\r\n",
    "        map(lambda cur_col: \"`{cur_col}`\".format(cur_col=cur_col),\r\n",
    "            sqlDF.columns)))\r\n",
    "    \r\n",
    "    #for max_group_count in list(range(1,9,2))+list(range(10,150,20)):\r\n",
    "    for max_group_count in [3]:\r\n",
    "        print(max_group_count)\r\n",
    "        \r\n",
    "        # groupby string attribute and filter out instances which only consider specific times\r\n",
    "        sqlDF_instances_to_consider = sqlDF.groupby(join_attributes).agg(count(join_attributes[0]).alias(\"count\")).where(col(\"count\") <= max_group_count)\r\n",
    "        resSqlDF = sqlDF.join(sqlDF_instances_to_consider, on=join_attributes, how='inner')\r\n",
    "\r\n",
    "        attr_variations = pair_permutations_ordered(intersecting_attr)\r\n",
    "        #print(attr_variations)\r\n",
    "\r\n",
    "        # selsect specific attr_variation with a specific attribute included\r\n",
    "        sel_attr = ['H','BB','X1B','X2B']\r\n",
    "        sel_attr_variations = list(\r\n",
    "            filter(lambda x: x[1] in sel_attr, attr_variations))\r\n",
    "        #print(sel_attr_variations)\r\n",
    "        #print(sel_attr_variations)\r\n",
    "        #print(len(sel_attr_variations))\r\n",
    "\r\n",
    "        #sel_attr_variations = [['H', \"H\"]]\r\n",
    "        \r\n",
    "        for index_attr, curr_item in enumerate(sel_attr_variations):\r\n",
    "            result_list=[]\r\n",
    "            #print(str(index_attr)+\"/\"+str(len(sel_attr_variations)))\r\n",
    "            first_attr = curr_item[0]\r\n",
    "            second_attr = curr_item[1]\r\n",
    "            # print(first_attr)\r\n",
    "            # print(second_attr)\r\n",
    "            emd = resSqlDF.select(emd_UDF(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr)),collect_list(\"`i.{second_attr}`\".format(second_attr=second_attr))).alias(\"EMD\")).collect()[0][\"EMD\"]\r\n",
    "            #print([outer,first_attr, inner, second_attr, max_group_count, float(emd)])\r\n",
    "            result_list.append([outer,first_attr, inner, second_attr, max_group_count, float(emd)])\r\n",
    "            resultDF = spark.createDataFrame(result_list).toDF(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\",\"COUNT\",\"EMD\")\r\n",
    "            resultDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}/emd_results_dist_sep_inst_appr4_maxgr{max_group_count}_{first_attr}_{second_attr}\".format(outer=outer, inner=inner, max_group_count=max_group_count, first_attr=first_attr, second_attr=second_attr))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calc EMD and COS just for selected combinations\r\n",
    "for index, curr_set in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "    if index > 1:\r\n",
    "        break\r\n",
    "    outer = curr_set[0]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        break\r\n",
    "    inner = curr_set[1]\r\n",
    "    print(outer,inner)\r\n",
    "    # find matching attributes to compare\r\n",
    "    join_attributes = list(\r\n",
    "        set(string_attributes[inner]) & set(string_attributes[outer]))\r\n",
    "    join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : f\"o.`{join_att}` = i.`{join_att}`\" ,\\\r\n",
    "                                           join_attributes))\r\n",
    "    intersecting_attr = list(\r\n",
    "        set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "    projection_list = \" , \".join(\r\n",
    "        map(lambda attr: f\"o.`{attr}` as `{attr}`\",\r\n",
    "            join_attributes)) + \" , \" + \" , \".join(\r\n",
    "                map(\r\n",
    "                    lambda attr:\r\n",
    "                    f\"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\",\r\n",
    "                    intersecting_attr))\r\n",
    "    sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\r\n",
    "                            inner+ \" i \" + join_condition+\")\")\r\n",
    "    # filter out null tupels with null values\r\n",
    "    sqlDF = sqlDF.dropna(\r\n",
    "        subset=list(map(lambda cur_col: f\"`{cur_col}`\", sqlDF.columns)))\r\n",
    "\r\n",
    "    \r\n",
    "    for max_group_count in [1,3,5,7,9]:\r\n",
    "        print(max_group_count)\r\n",
    "        \r\n",
    "        # groupby string attribute and filter out instances which only consider specific times\r\n",
    "        sqlDF_instances_to_consider = sqlDF.groupby(join_attributes).agg(count(join_attributes[0]).alias(\"count\")).where(col(\"count\") <= max_group_count)\r\n",
    "        resSqlDF = sqlDF.join(sqlDF_instances_to_consider, on=join_attributes, how='inner')\r\n",
    "\r\n",
    "        attr_variations = pair_permutations_ordered(intersecting_attr)\r\n",
    "        #print(attr_variations)\r\n",
    "\r\n",
    "        # selsect specific attr_variation with a specific attribute included\r\n",
    "        sel_attr = ['H','BB','X1B','X2B']\r\n",
    "        sel_attr_variations = list(\r\n",
    "            filter(lambda x: x[1] in sel_attr, attr_variations))\r\n",
    "        #print(sel_attr_variations)\r\n",
    "        #print(sel_attr_variations)\r\n",
    "        #print(len(sel_attr_variations))\r\n",
    "\r\n",
    "        #sel_attr_variations = [['H', \"H\"]]\r\n",
    "        result_list=[]\r\n",
    "        for index_attr, curr_item in enumerate(sel_attr_variations):\r\n",
    "            print(str(index_attr)+\"/\"+str(len(sel_attr_variations)))\r\n",
    "            first_attr = curr_item[0]\r\n",
    "            second_attr = curr_item[1]\r\n",
    "            # print(first_attr)\r\n",
    "            # print(second_attr)\r\n",
    "            #emd = wasserstein_distance(resSqlDF.select(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr))).collect()[0][0],resSqlDF.select(collect_list(\"`o.{second_attr}`\".format(second_attr=second_attr))).collect()[0][0])\r\n",
    "            emd = resSqlDF.select(emd_UDF(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr)),collect_list(\"`i.{second_attr}`\".format(second_attr=second_attr))).alias(\"EMD\")).collect()[0][\"EMD\"]\r\n",
    "            #print([outer,first_attr, inner, second_attr, max_group_count, float(emd)])\r\n",
    "            result_list.append([outer, first_attr, inner, second_attr, max_group_count, float(emd)])\r\n",
    "        resultDF = spark.createDataFrame(result_list).toDF(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\",\"COUNT\",\"EMD\")\r\n",
    "        resultDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"results/emd_results_sep_instances_appr4/{outer}_{inner}/emd_results_dist_sep_inst_appr4_maxgr{max_group_count}\".format(outer=outer, inner=inner, max_group_count=max_group_count))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Results of Join Similarity sep instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read different csv files and combine it to one ###\r\n",
    "for index, el in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "#for index, el in enumerate([[\"MLB_1\", \"MLB_60\"]]):\r\n",
    "    outer = el[0]\r\n",
    "    inner = el[1]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        continue\r\n",
    "    #if index < 2:\r\n",
    "        #continue\r\n",
    "    if os.path.isdir(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}\".format(outer=outer, inner=inner)):\r\n",
    "        print(outer,inner)\r\n",
    "        DF = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}/*/*.csv\".format(outer=outer, inner=inner)).toPandas()\r\n",
    "        DF.to_csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}.csv\".format(outer=outer, inner=inner),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/MLB_1_MLB_10.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"*\").where(col(\"INNER_ATTR\") == \"H\").where(col(\"count\") == 1).sort(\"EMD\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") < 10000).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"INNER_ATTR\") == \"X1B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"INNER_ATTR\") == \"X2B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emds = []\r\n",
    "for i in range(101,0,-1):\r\n",
    "    #print(i)\r\n",
    "    emd = df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") <= i).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").select(\"avg(EMD)\").where(col(\"OUTER_ATTR\")==\"H\").where(col(\"INNER_ATTR\")==\"H\").collect()[0][\"avg(EMD)\"]\r\n",
    "    emds.append(emd)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20,10))\r\n",
    "\r\n",
    "plt.xticks(ticks=range(0,104,2))\r\n",
    "plt.grid(True, axis='both', which='both')\r\n",
    "plt.title(\"Average EMDs on groups with different Instance-Counts \")\r\n",
    "\r\n",
    "ax1.set_xlabel(\"Group Count\")\r\n",
    "ax1.set_ylabel(\"Avg(EMD)\")\r\n",
    "ax1.plot(range(101, 0,-1), emds, color=\"blue\", marker=\"o\", label=\"\")\r\n",
    "\r\n",
    "ax1.legend()\r\n",
    "\r\n",
    "#ax1.tick_params(axis='y', labelcolor=color)\r\n",
    "\r\n",
    "# ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis\r\n",
    "\r\n",
    "\r\n",
    "# ax2.set_ylabel(\"COS-Distance\", color=color)\r\n",
    "# ax2.plot(resultDF_H_H_join.INNER, resultDF_H_H_join.COS, color=color)\r\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/MLB_1_MLB_11.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").count()\r\n",
    "\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_12.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_14.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_60.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"INNER_ATTR\") == \"BB\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcing EMD for colum H with differen max number of group elements\r\n",
    "selected_col = \"H\"\r\n",
    "max_number_goup_el = 3\r\n",
    "results = [[],[]]\r\n",
    "for index, el in enumerate(combinations(list_of_all_MLB_tables, 2)):\r\n",
    "        outer = el[0]\r\n",
    "        inner = el[1]\r\n",
    "        if outer != \"MLB_1\":\r\n",
    "            continue\r\n",
    "        if os.path.isfile(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner)) == False:\r\n",
    "            #print(inner)\r\n",
    "            continue\r\n",
    "        df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "        df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "        if df.select(\"*\").where(col(\"OUTER_ATTR\") == selected_col).count() == 0 or df.select(\"*\").where(col(\"INNER_ATTR\") == selected_col).count() == 0:\r\n",
    "            continue\r\n",
    "        print(outer,inner)\r\n",
    "        avg_emd = df.select(\"*\").where(col(\"OUTER_ATTR\") == selected_col).where(col(\"INNER_ATTR\") == selected_col).where(col(\"count\") <= max_number_goup_el).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").collect()[0][\"avg(EMD)\"]\r\n",
    "        results[0].append(inner)\r\n",
    "        results[1].append(avg_emd)\r\n",
    "pickle.dump(results, open(\"/semantic_data_lake/semantic_data_lake/results/emd_result_dist_calcs_sep_instances_group_max{number}_BB.p\".format(number=max_number_goup_el),\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results, open(\"/semantic_data_lake/semantic_data_lake/results/emd_result_dist_calcs_sep_instances_group_max1.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for label results of col\r\n",
    "selected_col = \"X1B\"\r\n",
    "max_number_goup_el = 3\r\n",
    "for index, el in enumerate(combinations(list_of_all_MLB_tables, 2)):\r\n",
    "        outer = el[0]\r\n",
    "        inner = el[1]\r\n",
    "        if outer != \"MLB_1\":\r\n",
    "            continue\r\n",
    "        if os.path.isfile(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}.csv\".format(outer=outer, inner=inner)) == False:\r\n",
    "            #print(inner)\r\n",
    "            continue\r\n",
    "        df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "        df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "        if df.select(\"*\").where(col(\"OUTER_ATTR\") == selected_col).count() == 0 or df.select(\"*\").where(col(\"INNER_ATTR\") == selected_col).count() == 0:\r\n",
    "            continue\r\n",
    "        print(outer,inner)\r\n",
    "        df.select(\"*\").where(col(\"INNER_ATTR\") == selected_col).where(col(\"count\") == max_number_goup_el).sort(\"EMD\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evluate Labeling with join similarity on sep instances\r\n",
    "def classification_report_labeling(cols_to_label, max_group_count:int):\r\n",
    "    #col_to_label = \"H\"\r\n",
    "    #max_group_count = 1\r\n",
    "    true_labels = []\r\n",
    "    pred_labels = []\r\n",
    "    for column in cols_to_label:\r\n",
    "        for index, el in enumerate(combinations(list_of_all_MLB_tables, 2)):\r\n",
    "            outer = el[0]\r\n",
    "            inner = el[1]\r\n",
    "            if outer != \"MLB_1\":\r\n",
    "                continue\r\n",
    "            if os.path.isfile(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}.csv\".format(outer=outer, inner=inner)) == False:\r\n",
    "                #print(inner)\r\n",
    "                continue\r\n",
    "            df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "            df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "            if df.select(\"*\").where(col(\"INNER_ATTR\") == column).count() == 0:\r\n",
    "                continue\r\n",
    "            pred_label = df.select(\"*\").where(col(\"INNER_ATTR\") == column).where(col(\"count\") == max_group_count).sort(\"EMD\").collect()[0][\"OUTER_ATTR\"]\r\n",
    "            #print(pred_label)\r\n",
    "            true_labels.append(column)\r\n",
    "            pred_labels.append(pred_label)\r\n",
    "        \r\n",
    "    class_report_dic = classification_report(true_labels, pred_labels, output_dict=True)\r\n",
    "    return class_report_dic\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_labeling([\"H\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_labeling([\"X2B\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\r\n",
    "for group_count in [1,3]:\r\n",
    "    print(group_count)\r\n",
    "    result = {}\r\n",
    "    result[\"group_count\"] = group_count\r\n",
    "    result[\"classification_report\"] = classification_report_labeling([\"X2B\"], group_count)\r\n",
    "    results.append(result)\r\n",
    "pickle.dump(results,open(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/labeling_performance_report_X2B.p\",\"wb\"))\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots of the labeling performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pickle\r\n",
    "import sys\r\n",
    "sys.path.insert(0, \"D:\\\\semantic_data_lake\\\\semantic_data_lake\")\r\n",
    "\r\n",
    "import matplotlib\r\n",
    "\r\n",
    "from pyspark import SparkConf, SparkContext\r\n",
    "from pyspark.sql import SparkSession, DataFrame\r\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType\r\n",
    "from pyspark.sql.functions import udf, col, pandas_udf, PandasUDFType, collect_list, count, avg, lit\r\n",
    "\r\n",
    "column = \"overall\"\r\n",
    "\r\n",
    "results = pickle.load(open(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances_appr4/labeling_performance_report_\"+column+\".p\",\"rb\"))\r\n",
    "\r\n",
    "group_count = []\r\n",
    "f1_scores = []\r\n",
    "precisions = []\r\n",
    "recalls = []\r\n",
    "\r\n",
    "for el in results:\r\n",
    "    group_count.append(el[\"group_count\"])\r\n",
    "    f1_scores.append(el[\"classification_report\"][\"weighted avg\"][\"f1-score\"])\r\n",
    "    precisions.append(el[\"classification_report\"][\"weighted avg\"][\"precision\"])\r\n",
    "    recalls.append(el[\"classification_report\"][\"weighted avg\"][\"recall\"])\r\n",
    "\r\n",
    "fig, ax1 = plt.subplots(figsize=(20,10))\r\n",
    "\r\n",
    "plt.xticks(list(range(1,9,2))+list(range(10,150,20)),rotation=90)\r\n",
    "plt.grid(True, axis='both', which='both')\r\n",
    "plt.title(\"Performance Scores for Column \"+column)\r\n",
    "\r\n",
    "ax1.set_xlabel(\"max number of group elements considered\")\r\n",
    "ax1.set_ylabel(\"performance scores\")\r\n",
    "ax1.plot(group_count, f1_scores, color=\"red\", marker=\"o\", label=\"f1-score\")\r\n",
    "ax1.plot(group_count, precisions, color=\"blue\", marker=\"o\", label=\"precision\")\r\n",
    "ax1.plot(group_count, recalls, color=\"green\", marker=\"o\", label=\"recall\")\r\n",
    "\r\n",
    "\r\n",
    "# zip joins x and y coordinates in pairs\r\n",
    "for x,y in zip(group_count, f1_scores):\r\n",
    "\r\n",
    "    label = \"{:.2f}\".format(y)\r\n",
    "\r\n",
    "    plt.annotate(label, # this is the text\r\n",
    "                 (x,y), # this is the point to label\r\n",
    "                 textcoords=\"offset points\", # how to position the text\r\n",
    "                 xytext=(0,10), # distance from text to points (x,y)\r\n",
    "                 ha='center') # horizontal alignment can be left, right or center\r\n",
    "\r\n",
    "ax1.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker pull csturm/pyspark-notebook\r\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57d51e845e90cb7046bb59f8f3e568dcb7335eb60d8163d91d0093f69686e60"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}