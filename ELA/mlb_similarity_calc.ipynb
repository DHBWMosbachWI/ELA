{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "sys.path.insert(0, \"D:\\\\semantic_data_lake\\\\semantic_data_lake\")\r\n",
    "from pyspark import SparkConf, SparkContext\r\n",
    "from pyspark.sql import SparkSession, DataFrame\r\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType\r\n",
    "from pyspark.sql.functions import udf, col, pandas_udf, PandasUDFType, collect_list, count, avg, lit\r\n",
    "from scipy.stats import wasserstein_distance\r\n",
    "from numpy import asarray\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pickle\r\n",
    "\r\n",
    "from itertools import permutations, combinations, combinations_with_replacement\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from helper_functions import print_df_to_html, translate_header_file_to_list, variations, pair_permutations_ordered, translate_datatype_file_to_list, cast_datatypes, check_attribute_completeness, compare_schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Config\r\n",
    "conf = SparkConf()\r\n",
    "conf.set(\"spark.executor.instances\",\"4\")\r\n",
    "conf.set(\"spark.executor.cores\",\"4\")\r\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\r\n",
    "conf.set(\"spark.driver.memory\", \"8g\")\r\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\r\n",
    "conf.set(\"spark.memory.offHeap.size\", \"16g\")\r\n",
    "conf.setMaster(\"local[*]\")\r\n",
    "conf.setAppName(\"MLB-similarity-calc\")\r\n",
    "# create a SparkSession\r\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Register UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=FloatType())\r\n",
    "def emd_UDF(col1, col2) -> FloatType:\r\n",
    "    return float(wasserstein_distance(col1, col2))\r\n",
    "\r\n",
    "spark.udf.register(\"emd_UDF\", emd_UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_REL_PATH = \"semantic_data_lake/semantic_data_lake/data/benchmark/\"\r\n",
    "BENCHMARK_REL_PATH = \"data/benchmark/\"\r\n",
    "\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_20\", \"MLB_15\"]\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_20\"]\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_10\"]\r\n",
    "list_of_all_MLB_tables = [\r\n",
    "    \"MLB_1\", \"MLB_10\", \"MLB_11\", \"MLB_12\", \"MLB_13\", \"MLB_14\", \"MLB_15\",\r\n",
    "    \"MLB_16\", \"MLB_17\", \"MLB_18\", \"MLB_19\", \"MLB_2\", \"MLB_20\", \"MLB_21\",\r\n",
    "    \"MLB_22\", \"MLB_23\", \"MLB_24\", \"MLB_25\", \"MLB_26\", \"MLB_27\", \"MLB_28\",\r\n",
    "    \"MLB_29\", \"MLB_3\", \"MLB_30\", \"MLB_31\", \"MLB_32\", \"MLB_33\", \"MLB_34\",\r\n",
    "    \"MLB_35\", \"MLB_36\", \"MLB_37\", \"MLB_38\", \"MLB_39\", \"MLB_4\", \"MLB_40\",\r\n",
    "    \"MLB_41\", \"MLB_42\", \"MLB_43\", \"MLB_44\", \"MLB_45\", \"MLB_46\", \"MLB_47\",\r\n",
    "    \"MLB_48\", \"MLB_49\", \"MLB_5\", \"MLB_50\", \"MLB_51\", \"MLB_52\", \"MLB_53\",\r\n",
    "    \"MLB_54\", \"MLB_55\", \"MLB_56\", \"MLB_57\", \"MLB_58\", \"MLB_59\", \"MLB_6\",\r\n",
    "    \"MLB_60\", \"MLB_61\", \"MLB_62\", \"MLB_63\", \"MLB_64\", \"MLB_65\", \"MLB_66\",\r\n",
    "    \"MLB_67\", \"MLB_68\", \"MLB_7\", \"MLB_8\", \"MLB_9\"\r\n",
    "]\r\n",
    "#list_of_MLB_join_candidate_pairs = [ (\"MLB_1\",\"MLB_12\"), (\"MLB_1\",\"MLB_13\"), (\"MLB_1\",\"MLB_14\")  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = BENCHMARK_REL_PATH + \"MLB/\"\r\n",
    "sample = False\r\n",
    "\r\n",
    "# dict of string attributes for each table\r\n",
    "string_attributes = {}\r\n",
    "numeric_attributes = {}\r\n",
    "for table_name in list_of_all_MLB_tables:\r\n",
    "    if sample:\r\n",
    "        data_file = file_path + \"samples/\" + table_name + \".sample\" + \".csv\"\r\n",
    "    else:\r\n",
    "        data_file = file_path + table_name + \".csv\"\r\n",
    "    header_file = file_path + \"samples/\" + table_name + \".header.csv\"\r\n",
    "    datatype_file = file_path + \"samples/\" + table_name + \".datatypes.csv\"\r\n",
    "    # create a DataFrame using an ifered Schema\r\n",
    "    orig_df = spark.read.option(\"header\", \"false\") \\\r\n",
    "    .option(\"inferSchema\", \"true\") \\\r\n",
    "    .option(\"delimiter\", \"|\") \\\r\n",
    "    .csv(data_file).toDF(*translate_header_file_to_list(header_file))\r\n",
    "    df = cast_datatypes(datatype_file, orig_df)\r\n",
    "    # compare_schemas(orig_df, df)\r\n",
    "    df.createOrReplaceTempView(table_name)\r\n",
    "    string_attributes[table_name] = list(filter(lambda x : not x.startswith(\"Calculation\"), \\\r\n",
    "                                    map(lambda x : x[0], filter(lambda tupel: tupel[1] == 'string' ,df.dtypes))))\r\n",
    "    numeric_attributes[table_name] = list(filter(lambda x : not x.startswith(\"Calculation\"), \\\r\n",
    "                                       map(lambda x : x[0], \\\r\n",
    "                                           filter(lambda tupel: tupel[1] == 'double' or \\\r\n",
    "                                           tupel[1] == 'int' or tupel[1].startswith('decimal'),df.dtypes))))\r\n",
    "    check_attribute_completeness(df.columns, string_attributes[table_name],\r\n",
    "                                 numeric_attributes[table_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, curr_set in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "    if index <= 9:\r\n",
    "        continue\r\n",
    "    outer = curr_set[0]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        continue\r\n",
    "    inner = curr_set[1]\r\n",
    "    print(outer,inner)\r\n",
    "    intersecting_attr = list(set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "    #print(intersecting_attr)\r\n",
    "    projection_list = \" , \".join(map(lambda attr: f\"`{attr}` as `{attr}`\",intersecting_attr))\r\n",
    "    #print(projection_list)\r\n",
    "    # read outer and inner table into sqlDF\r\n",
    "    sqlDFOuter = spark.sql(\"SELECT \"+ projection_list+\" FROM {outer}\".format(outer=outer))\r\n",
    "    sqlDFInner = spark.sql(\"SELECT \"+ projection_list+\" FROM {inner}\".format(inner=inner))\r\n",
    "\r\n",
    "    # filter out null tupels with null values\r\n",
    "    sqlDFOuter = sqlDFOuter.dropna(\r\n",
    "        subset=list(map(lambda cur_col: f\"`{cur_col}`\", sqlDFOuter.columns)))\r\n",
    "    # filter out null tupels with null values\r\n",
    "    sqlDFInner = sqlDFInner.dropna(\r\n",
    "        subset=list(map(lambda cur_col: f\"`{cur_col}`\", sqlDFInner.columns)))\r\n",
    "\r\n",
    "    attr_variations = pair_permutations_ordered(intersecting_attr)\r\n",
    "\r\n",
    "    # selsect specific attr_variation with a specific attribute included\r\n",
    "    sel_attr = ['H','BB','X1B','X2B']\r\n",
    "    sel_attr_variations = list(\r\n",
    "         filter(lambda x: x[1] in sel_attr, attr_variations))\r\n",
    "    \r\n",
    "    #print(sqlDFOuter.select(collect_list(\"`year`\")).collect()[0][0])\r\n",
    "    result_list =[]\r\n",
    "    for index_attr, curr_item in enumerate(sel_attr_variations):\r\n",
    "        #print(str(index_attr)+\"/\"+str(len(sel_attr_variations)))\r\n",
    "        first_attr = curr_item[0]\r\n",
    "        second_attr = curr_item[1]\r\n",
    "        #print(first_attr,second_attr)\r\n",
    "        emd = wasserstein_distance(sqlDFOuter.select(collect_list(\"`{first_attr}`\".format(first_attr=first_attr))).collect()[0][0],sqlDFInner.select(collect_list(\"`{second_attr}`\".format(second_attr=second_attr))).collect()[0][0])\r\n",
    "        result_list.append([outer,first_attr,inner,second_attr,float(emd)])\r\n",
    "    if len(result_list)==0:\r\n",
    "        continue\r\n",
    "    resultDF = spark.createDataFrame(result_list).toDF(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\",\"EMD\")\r\n",
    "    resultDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"results/no_join/{outer}_{inner}\".format(outer=outer, inner=inner))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum Results and save as one csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, el in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "#for index, el in enumerate([[\"MLB_1\", \"MLB_60\"]]):\r\n",
    "    outer = el[0]\r\n",
    "    inner = el[1]\r\n",
    "    #if index < 2:\r\n",
    "        #continue\r\n",
    "    print(outer,inner)\r\n",
    "    if os.path.isdir(\"results/no_join/{outer}_{inner}\".format(outer=outer, inner=inner)):\r\n",
    "        DF = spark.read.option(\"header\", \"true\").csv(\"results/no_join/{outer}_{inner}/*.csv\".format(outer=outer, inner=inner)).toPandas()\r\n",
    "        DF.to_csv(\"results/no_join/{outer}_{inner}.csv\".format(outer=outer, inner=inner),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure Labeling Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+------+----------+------------------+\n",
      "|OUTER|       OUTER_ATTR| INNER|INNER_ATTR|               EMD|\n",
      "+-----+-----------------+------+----------+------------------+\n",
      "|MLB_1|              wRC|MLB_10|         H|1.5363882628460028|\n",
      "|MLB_1|              PU.|MLB_10|         H| 2.163042541809402|\n",
      "|MLB_1|               LD|MLB_10|         H|2.2012191644286956|\n",
      "|MLB_1|               PU|MLB_10|         H|2.3257288597482932|\n",
      "|MLB_1|              X2B|MLB_10|         H| 2.338850993432464|\n",
      "|MLB_1|             wRAA|MLB_10|         H| 3.502819133166917|\n",
      "|MLB_1|             GIDP|MLB_10|         H|3.7706354615339515|\n",
      "|MLB_1|               HR|MLB_10|         H|3.8341132417398884|\n",
      "|MLB_1|              X1B|MLB_10|         H| 3.871509437983378|\n",
      "|MLB_1|               SF|MLB_10|         H|  4.33725627885215|\n",
      "|MLB_1|               SH|MLB_10|         H|4.3760011208004475|\n",
      "|MLB_1|              X3B|MLB_10|         H| 4.385526644318592|\n",
      "|MLB_1|              SLG|MLB_10|         H| 4.441155459265172|\n",
      "|MLB_1|Number of Records|MLB_10|         H| 4.513382751605004|\n",
      "|MLB_1|             wOBA|MLB_10|         H| 4.526635555793893|\n",
      "|MLB_1|              AVG|MLB_10|         H|  4.54610872412716|\n",
      "|MLB_1|              OBP|MLB_10|         H| 4.548431340796807|\n",
      "|MLB_1|            BABIP|MLB_10|         H| 4.553735107199799|\n",
      "|MLB_1|              ISO|MLB_10|         H| 4.625205895301509|\n",
      "|MLB_1|              HBP|MLB_10|         H| 4.754206825830593|\n",
      "+-----+-----------------+------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "'wRC'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outer = \"MLB_1\"\r\n",
    "inner = \"MLB_10\"\r\n",
    "col_to_label = \"H\"\r\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"results/no_join/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "df.select(\"*\").where(col(\"INNER_ATTR\") == col_to_label).sort(\"EMD\").show()\r\n",
    "df.select(\"*\").where(col(\"INNER_ATTR\") == col_to_label).sort(\"EMD\").collect()[0][\"OUTER_ATTR\"]\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evluate Labeling with join similarity on sep instances\r\n",
    "def classification_report_labeling(cols_to_label):\r\n",
    "    #col_to_label = \"H\"\r\n",
    "    #max_group_count = 1\r\n",
    "    true_labels = []\r\n",
    "    pred_labels = []\r\n",
    "    for column in cols_to_label:\r\n",
    "        for index, el in enumerate(combinations(list_of_all_MLB_tables, 2)):\r\n",
    "            outer = el[0]\r\n",
    "            inner = el[1]\r\n",
    "            if outer != \"MLB_1\":\r\n",
    "                continue\r\n",
    "            if os.path.isfile(\"results/no_join/{outer}_{inner}.csv\".format(outer=outer, inner=inner)) == False:\r\n",
    "                #print(inner)\r\n",
    "                continue\r\n",
    "            df = spark.read.option(\"header\", \"true\").csv(\"results/no_join/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "            df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "            if df.select(\"*\").where(col(\"INNER_ATTR\") == column).count() == 0:\r\n",
    "                continue\r\n",
    "            pred_label = df.select(\"*\").where(col(\"INNER_ATTR\") == column).sort(\"EMD\").collect()[0][\"OUTER_ATTR\"]\r\n",
    "            #print(pred_label)\r\n",
    "            true_labels.append(column)\r\n",
    "            pred_labels.append(pred_label)\r\n",
    "        \r\n",
    "    class_report_dic = classification_report(true_labels, pred_labels, output_dict=True)\r\n",
    "    return class_report_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'BB': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 52},\n 'BB.': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'FB': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'FB.': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'GIDP': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'H': {'precision': 0.5862068965517241,\n  'recall': 0.2982456140350877,\n  'f1-score': 0.3953488372093023,\n  'support': 57},\n 'HR': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'ISO': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'LD': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'PA': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'PU': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'PU.': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'SF': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'TB': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'X1B': {'precision': 0.6538461538461539,\n  'recall': 0.2982456140350877,\n  'f1-score': 0.40963855421686746,\n  'support': 57},\n 'X2B': {'precision': 0.6410256410256411,\n  'recall': 0.43859649122807015,\n  'f1-score': 0.5208333333333334,\n  'support': 57},\n 'wRC': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'accuracy': 0.2645739910313901,\n 'macro avg': {'precision': 0.11065168773079523,\n  'recall': 0.060887512899896794,\n  'f1-score': 0.07798945439761783,\n  'support': 223},\n 'weighted avg': {'precision': 0.4808138359243972,\n  'recall': 0.2645739910313901,\n  'f1-score': 0.3388869117098282,\n  'support': 223}}"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\Anaconda3\\envs\\semantic_data_lake\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'BB': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 52},\n 'BB.': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'FB': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'FB.': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'GIDP': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'H': {'precision': 0.5862068965517241,\n  'recall': 0.2982456140350877,\n  'f1-score': 0.3953488372093023,\n  'support': 57},\n 'HR': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'ISO': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'LD': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'PA': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'PU': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'PU.': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'SF': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'TB': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'X1B': {'precision': 0.6538461538461539,\n  'recall': 0.2982456140350877,\n  'f1-score': 0.40963855421686746,\n  'support': 57},\n 'X2B': {'precision': 0.6410256410256411,\n  'recall': 0.43859649122807015,\n  'f1-score': 0.5208333333333334,\n  'support': 57},\n 'wRC': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0},\n 'accuracy': 0.2645739910313901,\n 'macro avg': {'precision': 0.11065168773079523,\n  'recall': 0.060887512899896794,\n  'f1-score': 0.07798945439761783,\n  'support': 223},\n 'weighted avg': {'precision': 0.4808138359243972,\n  'recall': 0.2645739910313901,\n  'f1-score': 0.3388869117098282,\n  'support': 223}}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report_labeling([\"H\",\"BB\",\"X1B\",\"X2B\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('semantic_data_lake': conda)",
   "name": "python388jvsc74a57bd019be5c547be00959d458ecd8ae5a0aaaa8ac707411755967c8115f31bdf43a0d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}