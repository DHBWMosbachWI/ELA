{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\r\n",
    "import sys\r\n",
    "sys.path.insert(0, \"D:\\\\semantic_data_lake\\\\semantic_data_lake\")\r\n",
    "from pyspark import SparkConf, SparkContext\r\n",
    "from pyspark.sql import SparkSession, DataFrame\r\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DoubleType\r\n",
    "from pyspark.sql.functions import udf, col, pandas_udf, PandasUDFType, collect_list, count, avg, lit\r\n",
    "from scipy.stats import wasserstein_distance\r\n",
    "from numpy import asarray\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pickle\r\n",
    "\r\n",
    "from itertools import permutations, combinations, combinations_with_replacement\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from helper_functions import print_df_to_html, translate_header_file_to_list, variations, pair_permutations_ordered, translate_datatype_file_to_list, cast_datatypes, check_attribute_completeness, compare_schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Spark Config\r\n",
    "conf = SparkConf()\r\n",
    "conf.set(\"spark.executor.instances\",\"4\")\r\n",
    "conf.set(\"spark.executor.cores\",\"4\")\r\n",
    "conf.set(\"spark.executor.memory\", \"8g\")\r\n",
    "conf.set(\"spark.driver.memory\", \"8g\")\r\n",
    "conf.set(\"spark.memory.offHeap.enabled\", \"true\")\r\n",
    "conf.set(\"spark.memory.offHeap.size\", \"16g\")\r\n",
    "conf.setMaster(\"local[*]\")\r\n",
    "conf.setAppName(\"MLB-similarity-calc\")\r\n",
    "# create a SparkSession\r\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Register UDF  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=FloatType())\r\n",
    "def emd_UDF(col1, col2) -> FloatType:\r\n",
    "    return float(wasserstein_distance(col1, col2))\r\n",
    "\r\n",
    "spark.udf.register(\"emd_UDF\", emd_UDF)\r\n",
    "\r\n",
    "# sqlDF_MLB_1 = spark.sql(\"SELECT * from MLB_1\")\r\n",
    "# sqlDF_MLB_1.groupby([\"batter_name\", \"teamname\", \"parentteam\", \"league\"]).agg(\r\n",
    "#     emd_UDF(collect_list(\"H\"), collect_list(\"H\")).alias(\"EMD\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_REL_PATH = \"semantic_data_lake/semantic_data_lake/data/benchmark/\"\r\n",
    "BENCHMARK_REL_PATH = \"data/benchmark/\"\r\n",
    "\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_20\", \"MLB_15\"]\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_20\"]\r\n",
    "#list_of_all_MLB_tables = [ \"MLB_1\", \"MLB_10\"]\r\n",
    "list_of_all_MLB_tables = [\r\n",
    "    \"MLB_1\", \"MLB_10\", \"MLB_11\", \"MLB_12\", \"MLB_13\", \"MLB_14\", \"MLB_15\",\r\n",
    "    \"MLB_16\", \"MLB_17\", \"MLB_18\", \"MLB_19\", \"MLB_2\", \"MLB_20\", \"MLB_21\",\r\n",
    "    \"MLB_22\", \"MLB_23\", \"MLB_24\", \"MLB_25\", \"MLB_26\", \"MLB_27\", \"MLB_28\",\r\n",
    "    \"MLB_29\", \"MLB_3\", \"MLB_30\", \"MLB_31\", \"MLB_32\", \"MLB_33\", \"MLB_34\",\r\n",
    "    \"MLB_35\", \"MLB_36\", \"MLB_37\", \"MLB_38\", \"MLB_39\", \"MLB_4\", \"MLB_40\",\r\n",
    "    \"MLB_41\", \"MLB_42\", \"MLB_43\", \"MLB_44\", \"MLB_45\", \"MLB_46\", \"MLB_47\",\r\n",
    "    \"MLB_48\", \"MLB_49\", \"MLB_5\", \"MLB_50\", \"MLB_51\", \"MLB_52\", \"MLB_53\",\r\n",
    "    \"MLB_54\", \"MLB_55\", \"MLB_56\", \"MLB_57\", \"MLB_58\", \"MLB_59\", \"MLB_6\",\r\n",
    "    \"MLB_60\", \"MLB_61\", \"MLB_62\", \"MLB_63\", \"MLB_64\", \"MLB_65\", \"MLB_66\",\r\n",
    "    \"MLB_67\", \"MLB_68\", \"MLB_7\", \"MLB_8\", \"MLB_9\"\r\n",
    "]\r\n",
    "#list_of_MLB_join_candidate_pairs = [ (\"MLB_1\",\"MLB_12\"), (\"MLB_1\",\"MLB_13\"), (\"MLB_1\",\"MLB_14\")  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(combinations(list_of_all_MLB_tables,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_path = BENCHMARK_REL_PATH + \"MLB/\"\n",
    "sample = False\n",
    "\n",
    "# dict of string attributes for each table\n",
    "string_attributes = {}\n",
    "numeric_attributes = {}\n",
    "for table_name in list_of_all_MLB_tables:\n",
    "    if sample:\n",
    "        data_file = file_path + \"samples/\" + table_name + \".sample\" + \".csv\"\n",
    "    else:\n",
    "        data_file = file_path + table_name + \".csv\"\n",
    "    header_file = file_path + \"samples/\" + table_name + \".header.csv\"\n",
    "    datatype_file = file_path + \"samples/\" + table_name + \".datatypes.csv\"\n",
    "    # create a DataFrame using an ifered Schema\n",
    "    orig_df = spark.read.option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .csv(data_file).toDF(*translate_header_file_to_list(header_file))\n",
    "    df = cast_datatypes(datatype_file, orig_df)\n",
    "    # compare_schemas(orig_df, df)\n",
    "    df.createOrReplaceTempView(table_name)\n",
    "    string_attributes[table_name] = list(filter(lambda x : not x.startswith(\"Calculation\"), \\\n",
    "                                    map(lambda x : x[0], filter(lambda tupel: tupel[1] == 'string' ,df.dtypes))))\n",
    "    numeric_attributes[table_name] = list(filter(lambda x : not x.startswith(\"Calculation\"), \\\n",
    "                                       map(lambda x : x[0], \\\n",
    "                                           filter(lambda tupel: tupel[1] == 'double' or \\\n",
    "                                           tupel[1] == 'int' or tupel[1].startswith('decimal'),df.dtypes))))\n",
    "    check_attribute_completeness(df.columns, string_attributes[table_name],\n",
    "                                 numeric_attributes[table_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = \"MLB_1\"\r\n",
    "inner = \"MLB_10\"\r\n",
    "# find matching attributes to compare\r\n",
    "join_attributes = list(\r\n",
    "    set(string_attributes[outer]) & set(string_attributes[inner]))\r\n",
    "print(join_attributes)\r\n",
    "join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : f\"o.`{join_att}` = i.`{join_att}`\" ,\\\r\n",
    "                                           join_attributes))\r\n",
    "print(join_condition)\r\n",
    "intersecting_attr = list(\r\n",
    "    set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "intersecting_attr = list(\"H\")\r\n",
    "print(intersecting_attr)\r\n",
    "#create projection list\r\n",
    "projection_list = \" , \".join(\r\n",
    "    map(lambda attr: f\"o.`{attr}` as `{attr}`\", join_attributes)\r\n",
    ") + \" , \" + \" , \".join(\r\n",
    "    map(lambda attr: f\"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\",\r\n",
    "        intersecting_attr))\r\n",
    "print(projection_list)\r\n",
    "sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\r\n",
    "                        inner+ \" i \" + join_condition+\")\")\r\n",
    "sqlDF = sqlDF.dropna(\r\n",
    "    subset=list(map(lambda cur_col: f\"`{cur_col}`\", sqlDF.columns)))\r\n",
    "\r\n",
    "#sqlDF.groupby(join_attributes).agg(emd_UDF(collect_list(\"`o.H`\"),collect_list(\"`i.H`\")).alias(\"EMD\"), count(\"`i.H`\").alias(\"count\")).where(col(\"count\") < 10000).show()\r\n",
    "sqlDF.groupby(join_attributes).agg(emd_UDF(collect_list(\"`o.H`\"),collect_list(\"`i.H`\")).alias(\"EMD\"), count(\"`i.H`\").alias(\"count\")).where(col(\"count\") < 10000).select(avg(col(\"EMD\"))).show()\r\n",
    "#sqlDF.groupby(join_attributes).agg(emd_UDF(collect_list(\"`o.H`\"),collect_list(\"`i.H`\")).alias(\"EMD\"), count(\"`i.H`\").alias(\"count\")).where(col(\"count\") < 10).groupby().avg(\"EMD\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sep_instances = sqlDF.groupby(join_attributes).count().toPandas()\n",
    "df_sep_instances[df_sep_instances[\"count\"] <2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Calc EMD and COS just for selected combinations\r\n",
    "\r\n",
    "result_dist_calc = []\r\n",
    "\r\n",
    "for index, curr_set in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "    if index > 1:\r\n",
    "        break\r\n",
    "    outer = curr_set[0]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        break\r\n",
    "    inner = curr_set[1]\r\n",
    "    print(outer)\r\n",
    "    print(inner)\r\n",
    "    #print(index)\r\n",
    "    # find matching attributes to compare\r\n",
    "    join_attributes = list(\r\n",
    "        set(string_attributes[inner]) & set(string_attributes[outer]))\r\n",
    "    join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : f\"o.`{join_att}` = i.`{join_att}`\" ,\\\r\n",
    "                                           join_attributes))\r\n",
    "    intersecting_attr = list(\r\n",
    "        set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "    #print(intersecting_attr)\r\n",
    "    #create projection list\r\n",
    "    projection_list = \" , \".join(\r\n",
    "        map(lambda attr: f\"o.`{attr}` as `{attr}`\",\r\n",
    "            join_attributes)) + \" , \" + \" , \".join(\r\n",
    "                map(\r\n",
    "                    lambda attr:\r\n",
    "                    f\"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\",\r\n",
    "                    intersecting_attr))\r\n",
    "    sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\r\n",
    "                            inner+ \" i \" + join_condition+\")\")\r\n",
    "    # filter out null tupels with null values\r\n",
    "    sqlDF = sqlDF.dropna(\r\n",
    "        subset=list(map(lambda cur_col: f\"`{cur_col}`\", sqlDF.columns)))\r\n",
    "    # calculates null values in the table\r\n",
    "    #sqlDF.select([count(when(isnan(f\"`{c}`\") | col(f\"`{c}`\").isNull(), c)).alias(f\"`{c}`\") for c in sqlDF.columns]).show()\r\n",
    "\r\n",
    "    # calcultes basic statisitc for the attributes\r\n",
    "    #print_df_to_html(sqlDF.describe())\r\n",
    "\r\n",
    "    attr_variations = pair_permutations_ordered(intersecting_attr)\r\n",
    "    print(attr_variations)\r\n",
    "\r\n",
    "    # selsect specific attr_variation with a specific attribute included\r\n",
    "    sel_attr = ['H','BB','X1B','X2B']\r\n",
    "    sel_attr_variations = list(\r\n",
    "         filter(lambda x: x[1] in sel_attr, attr_variations))\r\n",
    "    print(sel_attr_variations)\r\n",
    "\r\n",
    "    #sel_attr_variations = [['H', \"H\"]]\r\n",
    "\r\n",
    "    for index_attr, curr_item in enumerate(sel_attr_variations):\r\n",
    "        print(str(index_attr)+\"/\"+str(len(sel_attr_variations)))\r\n",
    "        first_attr = curr_item[0]\r\n",
    "        second_attr = curr_item[1]\r\n",
    "        # print(first_attr)\r\n",
    "        # print(second_attr)\r\n",
    "        if index_attr == 0:\r\n",
    "            curDF = sqlDF.groupby(join_attributes).agg(emd_UDF(collect_list(f\"`o.{first_attr}`\"),collect_list(f\"`i.{second_attr}`\")).alias(\"EMD\"), count(\"`i.H`\").alias(\"count\")).select(col(\"EMD\"), col(\"count\"))\r\n",
    "            curDF = curDF.withColumn(\"OUTER\", lit(outer)).withColumn(\"OUTER_ATTR\", lit(first_attr)).withColumn(\"INNER\", lit(inner)).withColumn(\"INNER_ATTR\", lit(second_attr))\r\n",
    "        else:\r\n",
    "            newDF = sqlDF.groupby(join_attributes).agg(emd_UDF(collect_list(f\"`o.{first_attr}`\"),collect_list(f\"`i.{second_attr}`\")).alias(\"EMD\"), count(\"`i.H`\").alias(\"count\")).select(col(\"EMD\"), col(\"count\"))\r\n",
    "            newDF = newDF.withColumn(\"OUTER\", lit(outer)).withColumn(\"OUTER_ATTR\", lit(first_attr)).withColumn(\"INNER\", lit(inner)).withColumn(\"INNER_ATTR\", lit(second_attr))\r\n",
    "            curDF = curDF.union(newDF)\r\n",
    "        # ws_calc = sqlDF.groupby(join_attributes).agg(\r\n",
    "        #     emd_UDF(collect_list(f\"`o.{first_attr}`\"),\r\n",
    "        #             collect_list(f\"`i.{first_attr}`\")).alias(\r\n",
    "        #                 \"EMD\")).groupby().avg(\"EMD\").first()[0]\r\n",
    "        #curDF = sqlDF.groupby(join_attributes).agg(emd_UDF(collect_list(f\"`o.{first_attr}`\"),collect_list(f\"`i.{second_attr}`\")).alias(\"EMD\"), count(\"`i.H`\").alias(\"count\"))\r\n",
    "        #curDF = curDF.withColumn(\"OUTER\", lit(outer)).withColumn(\"OUTER_ATTR\", lit(first_attr)).withColumn(\"INNER\", lit(inner)).withColumn(\"INNER_ATTR\", lit(second_attr))\r\n",
    "        #first_second_projection = sqlDF.select(col(f\"`o.{first_attr}`\"),col(f\"`i.{second_attr}`\"))\r\n",
    "        #cos_calc = 0.0\r\n",
    "        #cos_sim = RowMatrix(first_second_projection.rdd.map(list)).columnSimilarities()\r\n",
    "        #if cos_sim.entries.count()> 0:\r\n",
    "        #    cos_calc = cos_sim.entries.first().value\r\n",
    "        #else:\r\n",
    "        # print(f\"Cos calc not possible {outer} {first_attr} {first_second_projection.dtypes[0]} {inner} {second_attr} {first_second_projection.dtypes[1]}\")\r\n",
    "        #    cos_calc = float(\"NaN\")\r\n",
    "    curDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"semantic_data_lake/semantic_data_lake/results/{outer}_{inner}/emd_results_dist_sep_inst_{outer}_{inner}\".format(outer=outer, inner=inner))\r\n",
    "#         result_dist_calc.append([outer, first_attr, inner, second_attr, \\\r\n",
    "#                                  float(ws_calc)])\r\n",
    "# result_dist_calcdf = spark.createDataFrame(result_dist_calc).toDF(\r\n",
    "#     \"OUTER\", \"OUTER_ATTR\", \"INNER\", \"INNER_ATTR\", \"EMD\")\r\n",
    "# result_dist_calcdf.coalesce(1).write.format(\"csv\").mode(\"overwrite\")\\\r\n",
    "#                     .option(\"header\",\"true\").save(\"results/emd_result_dist_calcs_sep_instances\")\r\n",
    "# result_dist_calcdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"results/emd_results_dist_sep_inst_{outer}_{inner}\".format(outer=outer, inner=inner))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDF.write.mode(\"overwrite\").option(\"header\", \"true\").parquet(\"semantic_data_lake/semantic_data_lake/results/emd_results_dist_sep_inst_par\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calc EMD and COS just for selected combinations\r\n",
    "\r\n",
    "result_dist_calc = []\r\n",
    "\r\n",
    "for index, curr_set in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "    #     if index > 1:\r\n",
    "    #         break\r\n",
    "    outer = curr_set[0]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        break\r\n",
    "    inner = curr_set[1]\r\n",
    "    print(outer,inner)\r\n",
    "    #print(inner)\r\n",
    "    #print(index)\r\n",
    "    # find matching attributes to compare\r\n",
    "    join_attributes = list(\r\n",
    "        set(string_attributes[inner]) & set(string_attributes[outer]))\r\n",
    "    join_condition = \"ON (\" + \" AND \".join(map(lambda join_att : f\"o.`{join_att}` = i.`{join_att}`\" ,\\\r\n",
    "                                           join_attributes))\r\n",
    "    intersecting_attr = list(\r\n",
    "        set(numeric_attributes[inner]) & set(numeric_attributes[outer]))\r\n",
    "    #print(intersecting_attr)\r\n",
    "    #create projection list\r\n",
    "    projection_list = \" , \".join(\r\n",
    "        map(lambda attr: f\"o.`{attr}` as `{attr}`\",\r\n",
    "            join_attributes)) + \" , \" + \" , \".join(\r\n",
    "                map(\r\n",
    "                    lambda attr:\r\n",
    "                    f\"o.`{attr}` as `o.{attr}` , i.`{attr}` as `i.{attr}`\",\r\n",
    "                    intersecting_attr))\r\n",
    "    sqlDF = spark.sql(\"SELECT \"+projection_list+\" FROM \" +outer +\" o JOIN \"+ \\\r\n",
    "                            inner+ \" i \" + join_condition+\")\")\r\n",
    "    # filter out null tupels with null values\r\n",
    "    sqlDF = sqlDF.dropna(\r\n",
    "        subset=list(map(lambda cur_col: f\"`{cur_col}`\", sqlDF.columns)))\r\n",
    "    # calculates null values in the table\r\n",
    "    #sqlDF.select([count(when(isnan(f\"`{c}`\") | col(f\"`{c}`\").isNull(), c)).alias(f\"`{c}`\") for c in sqlDF.columns]).show()\r\n",
    "\r\n",
    "    # calcultes basic statisitc for the attributes\r\n",
    "    #print_df_to_html(sqlDF.describe())\r\n",
    "\r\n",
    "    attr_variations = pair_permutations_ordered(intersecting_attr)\r\n",
    "    #print(attr_variations)\r\n",
    "\r\n",
    "    # selsect specific attr_variation with a specific attribute included\r\n",
    "    sel_attr = ['H','BB','X1B','X2B']\r\n",
    "    sel_attr_variations = list(\r\n",
    "         filter(lambda x: x[1] in sel_attr, attr_variations))\r\n",
    "    #print(sel_attr_variations)\r\n",
    "    #print(sel_attr_variations)\r\n",
    "    #print(len(sel_attr_variations))\r\n",
    "\r\n",
    "    #sel_attr_variations = [['H', \"H\"]]\r\n",
    "\r\n",
    "    for index_attr, curr_item in enumerate(sel_attr_variations):\r\n",
    "        #print(str(index_attr)+\"/\"+str(len(sel_attr_variations)))\r\n",
    "        first_attr = curr_item[0]\r\n",
    "        second_attr = curr_item[1]\r\n",
    "        # print(first_attr)\r\n",
    "        # print(second_attr)\r\n",
    "        curDF = sqlDF.groupby(join_attributes).agg(emd_UDF(collect_list(\"`o.{first_attr}`\".format(first_attr=first_attr)),collect_list(\"`i.{second_attr}`\".format(second_attr=second_attr))).alias(\"EMD\"),count(\"`i.H`\").alias(\"count\")).select(col(\"EMD\"), col(\"count\"))\r\n",
    "        curDF = curDF.withColumn(\"OUTER\", lit(outer)).withColumn(\"OUTER_ATTR\",lit(first_attr)).withColumn(\"INNER\", lit(inner)).withColumn(\"INNER_ATTR\", lit(second_attr))\r\n",
    "        curDF.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"/semantic_data_lake/results/{outer}_{inner}/emd_results_dist_sep_inst_{outer}_{first_attr}_{inner}_{second_attr}_\".format(outer=outer, first_attr=first_attr, inner=inner, second_attr=second_attr))\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dist_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Results of Join Similarity sep instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, el in enumerate(list(combinations(list_of_all_MLB_tables,2))):\r\n",
    "#for index, el in enumerate([[\"MLB_1\", \"MLB_60\"]]):\r\n",
    "    outer = el[0]\r\n",
    "    inner = el[1]\r\n",
    "    if outer != \"MLB_1\":\r\n",
    "        continue\r\n",
    "    #if index < 2:\r\n",
    "        #continue\r\n",
    "    if os.path.isdir(\"results/emd_results_sep_instances/{outer}_{inner}\".format(outer=outer, inner=inner)):\r\n",
    "        print(outer,inner)\r\n",
    "        DF = spark.read.option(\"header\", \"true\").csv(\"results/emd_results_sep_instances/{outer}_{inner}/*/*.csv\".format(outer=outer, inner=inner)).toPandas()\r\n",
    "        DF.to_csv(\"results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_10.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "df.select(\"*\").where(col(\"INNER_ATTR\") == \"H\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") < 10000).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"INNER_ATTR\") == \"X1B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"INNER_ATTR\") == \"X2B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emds = []\r\n",
    "for i in range(101,0,-1):\r\n",
    "    #print(i)\r\n",
    "    emd = df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") <= i).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").select(\"avg(EMD)\").where(col(\"OUTER_ATTR\")==\"H\").where(col(\"INNER_ATTR\")==\"H\").collect()[0][\"avg(EMD)\"]\r\n",
    "    emds.append(emd)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(20,10))\r\n",
    "\r\n",
    "plt.xticks(ticks=range(0,104,2))\r\n",
    "plt.grid(True, axis='both', which='both')\r\n",
    "plt.title(\"Average EMDs on groups with different Instance-Counts \")\r\n",
    "\r\n",
    "ax1.set_xlabel(\"Group Count\")\r\n",
    "ax1.set_ylabel(\"Avg(EMD)\")\r\n",
    "ax1.plot(range(101, 0,-1), emds, color=\"blue\", marker=\"o\", label=\"\")\r\n",
    "\r\n",
    "ax1.legend()\r\n",
    "\r\n",
    "#ax1.tick_params(axis='y', labelcolor=color)\r\n",
    "\r\n",
    "# ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis\r\n",
    "\r\n",
    "\r\n",
    "# ax2.set_ylabel(\"COS-Distance\", color=color)\r\n",
    "# ax2.plot(resultDF_H_H_join.INNER, resultDF_H_H_join.COS, color=color)\r\n",
    "# ax2.tick_params(axis='y', labelcolor=color)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_11.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").count()\r\n",
    "\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_12.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "#df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") < 20).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_14.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"BB\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/MLB_1_MLB_60.csv\")\r\n",
    "df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"H\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"INNER_ATTR\") == \"BB\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X1B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)\r\n",
    "df.select(\"*\").where(col(\"OUTER_ATTR\") == \"X2B\").where(col(\"count\") <= 1).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcing EMD for colum H with differen max number of group elements\r\n",
    "selected_col = \"BB\"\r\n",
    "max_number_goup_el = 1000000 \r\n",
    "results = [[],[]]\r\n",
    "for index, el in enumerate(combinations(list_of_all_MLB_tables, 2)):\r\n",
    "        outer = el[0]\r\n",
    "        inner = el[1]\r\n",
    "        if outer != \"MLB_1\":\r\n",
    "            continue\r\n",
    "        if os.path.isfile(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner)) == False:\r\n",
    "            #print(inner)\r\n",
    "            continue\r\n",
    "        df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "        df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "        if df.select(\"*\").where(col(\"OUTER_ATTR\") == selected_col).count() == 0 or df.select(\"*\").where(col(\"INNER_ATTR\") == selected_col).count() == 0:\r\n",
    "            continue\r\n",
    "        print(outer,inner)\r\n",
    "        avg_emd = df.select(\"*\").where(col(\"OUTER_ATTR\") == selected_col).where(col(\"INNER_ATTR\") == selected_col).where(col(\"count\") <= max_number_goup_el).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").collect()[0][\"avg(EMD)\"]\r\n",
    "        results[0].append(inner)\r\n",
    "        results[1].append(avg_emd)\r\n",
    "pickle.dump(results, open(\"/semantic_data_lake/semantic_data_lake/results/emd_result_dist_calcs_sep_instances_group_max{number}_BB.p\".format(number=max_number_goup_el),\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(results, open(\"/semantic_data_lake/semantic_data_lake/results/emd_result_dist_calcs_sep_instances_group_max1.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking for label results of col BB\r\n",
    "selected_col = \"X1B\"\r\n",
    "max_number_goup_el = 3\r\n",
    "for index, el in enumerate(combinations(list_of_all_MLB_tables, 2)):\r\n",
    "        outer = el[0]\r\n",
    "        inner = el[1]\r\n",
    "        if outer != \"MLB_1\":\r\n",
    "            continue\r\n",
    "        if os.path.isfile(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner)) == False:\r\n",
    "            #print(inner)\r\n",
    "            continue\r\n",
    "        df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "        df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "        if df.select(\"*\").where(col(\"OUTER_ATTR\") == selected_col).count() == 0 or df.select(\"*\").where(col(\"INNER_ATTR\") == selected_col).count() == 0:\r\n",
    "            continue\r\n",
    "        print(outer,inner)\r\n",
    "        df.select(\"*\").where(col(\"INNER_ATTR\") == selected_col).where(col(\"count\") <= max_number_goup_el).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evluate Labeling with join similarity on sep instances\r\n",
    "def classification_report_labeling(cols_to_label, max_group_count:int):\r\n",
    "    #col_to_label = \"H\"\r\n",
    "    #max_group_count = 1\r\n",
    "    true_labels = []\r\n",
    "    pred_labels = []\r\n",
    "    for column in cols_to_label:\r\n",
    "        for index, el in enumerate(combinations(list_of_all_MLB_tables, 2)):\r\n",
    "            outer = el[0]\r\n",
    "            inner = el[1]\r\n",
    "            if outer != \"MLB_1\":\r\n",
    "                continue\r\n",
    "            if os.path.isfile(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner)) == False:\r\n",
    "                #print(inner)\r\n",
    "                continue\r\n",
    "            df = spark.read.option(\"header\", \"true\").csv(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/{outer}_{inner}.csv\".format(outer=outer, inner=inner))\r\n",
    "            df = df.withColumn(\"EMD\",df[\"EMD\"].cast(DoubleType()))\r\n",
    "            if df.select(\"*\").where(col(\"INNER_ATTR\") == column).count() == 0:\r\n",
    "                continue\r\n",
    "            pred_label = df.select(\"*\").where(col(\"INNER_ATTR\") == column).where(col(\"count\") <= max_group_count).groupBy(\"OUTER\",\"OUTER_ATTR\",\"INNER\",\"INNER_ATTR\").avg(\"EMD\").alias(\"avg(EMD)\").sort(\"avg(EMD)\").collect()[0][\"OUTER_ATTR\"]\r\n",
    "            #print(pred_label)\r\n",
    "            true_labels.append(column)\r\n",
    "            pred_labels.append(pred_label)\r\n",
    "        \r\n",
    "    class_report_dic = classification_report(true_labels, pred_labels, output_dict=True)\r\n",
    "    return class_report_dic\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\r\n",
    "for group_count in list(range(1,9,2))+list(range(10,150,20)):\r\n",
    "    print(group_count)\r\n",
    "    result = {}\r\n",
    "    result[\"group_count\"] = group_count\r\n",
    "    result[\"classification_report\"] = classification_report_labeling([\"H\",\"BB\",\"X1B\",\"X2B\"], group_count)\r\n",
    "    results.append(result)\r\n",
    "pickle.dump(results,open(\"results/emd_results_sep_instances/labeling_performance_report_overall.p\",\"wb\"))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\r\n",
    "for group_count in list(range(1,9,2))+list(range(10,150,20)):\r\n",
    "    print(group_count)\r\n",
    "    result = {}\r\n",
    "    result[\"group_count\"] = group_count\r\n",
    "    result[\"classification_report\"] = classification_report_labeling(\"H\", group_count)\r\n",
    "    results.append(result)\r\n",
    "pickle.dump(results,open(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/labeling_performance_report_H.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\r\n",
    "for group_count in list(range(1,9,2))+list(range(10,150,20)):\r\n",
    "    print(group_count)\r\n",
    "    result = {}\r\n",
    "    result[\"group_count\"] = group_count\r\n",
    "    result[\"classification_report\"] = classification_report_labeling(\"BB\", group_count)\r\n",
    "    results.append(result)\r\n",
    "pickle.dump(results,open(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/labeling_performance_report_BB.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\r\n",
    "for group_count in list(range(1,9,2))+list(range(10,150,20)):\r\n",
    "    print(group_count)\r\n",
    "    result = {}\r\n",
    "    result[\"group_count\"] = group_count\r\n",
    "    result[\"classification_report\"] = classification_report_labeling(\"X1B\", group_count)\r\n",
    "    results.append(result)\r\n",
    "pickle.dump(results,open(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/labeling_performance_report_X1B.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\r\n",
    "for group_count in list(range(1,9,2))+list(range(10,150,20)):\r\n",
    "    print(group_count)\r\n",
    "    result = {}\r\n",
    "    result[\"group_count\"] = group_count\r\n",
    "    result[\"classification_report\"] = classification_report_labeling(\"X2B\", group_count)\r\n",
    "    results.append(result)\r\n",
    "pickle.dump(results,open(\"/semantic_data_lake/semantic_data_lake/results/emd_results_sep_instances/labeling_performance_report_X2B.p\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f57d51e845e90cb7046bb59f8f3e568dcb7335eb60d8163d91d0093f69686e60"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}